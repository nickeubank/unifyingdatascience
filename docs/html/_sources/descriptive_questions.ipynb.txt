{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Researcher Discretion in Descriptive Analysis\n",
    "\n",
    "More than anything, the goal of descriptive analyses is to extract patterns from otherwise incomprehensably messy data and present them to readers in a digestable manner. \n",
    "\n",
    "But intrinsic to this is choosing what constitutes a pattern (and is thus something worth reporting), and what information can be safely discarded. Descriptive analysis, in other words, *always* entails setting aside lots of information. After all, the only way to *not* set aside information would be to literally hand your original dataset over to the reader. All summarization for descriptive analysis -- be that reporting summary statistics and  plotting simple distributions, or running sophisticated clustering algorithms or dimensionality reduction tools -- therefore requires the researcher to decide what is and what is not important. \n",
    "\n",
    "But herein lies the hazard of descriptive analysis: readers are counting on *you*, the data scientist, to determine the meaningful patterns in the data and filter out presumably irrelevant variation. And thus it is incumbent upon you to ensure that you are highlighting *meaningful* patterns, and only setting aside things that do not matter.\n",
    "\n",
    "This may seem obvious, but in practice, this is often a place that students struggle. That is because most tools (be those functions for calculating means and standard deviations, or unsupervised machine learning algorithms like Principle Component Analyses (PCAs) or clustering algorithms) always \"work\" in the sense that they will always provide you with a result. And so these tools always make available the \"easy route\" of running the tool and reporting the result. \n",
    "\n",
    "But in doing so, it is easy to provide readers with a false sense of the world. Readers (in almost any context except during academic peer review), will generally assume that if you have decided to present a statistic, it must be meaningful, important, and informative. And so in choosing what to include in your reports, papers, or presentations, you have influence for how readers interpret your results. In other words, when presenting data, you aren't *just* communicating the specific values of a set of of statistics; you're also implicitly communicating a lot about the importance of those statistics for understanding the data.\n",
    "\n",
    "With that in mind, in this reader we will discuss **two questions** you should ask yourself whenever you are presenting a descriptive analysis:\n",
    "\n",
    "1. Am I faithfully representing the full data?\n",
    "2. Am I presenting the data that is most important?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Faithful Representation\n",
    "\n",
    "The fundamental goal of descriptive analysis is to take messy, incomprehensable data in its raw form, and extract from it a kind of summary of the most critical features of that data. When you are doing this, it is sometimes helpful to think of yourself as acting as a guide for the eventual reader of your analysis. Your goal, in the end, is to ensure that they *understand* the essential nature of your data without having to dive into themselves. \n",
    "\n",
    "With this in mind, it can be helpful when presenting data to ask yourself whether what you have presented gives a faithful representation of the data. \n",
    "\n",
    "Suppose, for example, you have been asked to evaluate the performance of an instructor. You recieve that professor's final exam scores, and you find that the average grade in the class was an 85. Great! Sounds like things are going well. \n",
    "\n",
    "But let's suppose that when you dig in a little more, you find that the actual grades in the class are bimodal -- all the grades in the class were either 100s or 0s. Now ask yourself: is just reporting the average grade in the class really communicating the nature of this data? I would argue not. \n",
    "\n",
    "This is a simple example, but the principle generalizes. Suppose, for example, I ran a linear model that showed that income rose with age, and that a person 10 years older than another likely earns \\$10,000 more on average. Imagine, in your mind, what you think of when you hear that. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now supposed I showed you the following plot, where $x_2$ is age in 5 year bins and $y_2$ is income in 10 thousand dollar increments: \n",
    "\n",
    "![anscombes_quartet_number_2](images/anscombes_2.png)\n",
    "\n",
    "Yup... that linear fit may be what I told you it was, but clearly the data is doing something not captured in a simple linear correlation. That's because linear models estimate linear trends; applied to non-linear data they still give you an answer, but I don't think anyone would say that telling the reader that people who are ten years older earn \\$10,000 more on average is faithfully representing the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So how do you protect yourself from these kind of errors? The specific answer often depends on the method in question (e.g. for linear regressions, you might want to look at residual plots; for simple means you might want to plot a histogram), but a general rule of thumb is that **you should always do analyses that go beyond what you're presenting to the reader.** Plot your data. See if your mean and median are radically different. Check your residual plots. Correlate your primary variable of interest with other variables just to see if there's some story there you're missing. Basically until you've explored your data to the point that you're not finding anything new and important during your explorations (i.e. until your new results are boring and stop surprising you), you probably don't know enough about the data to know if your summary statistics are faithfully representing the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing What To Present\n",
    "\n",
    "OK... by now you've probably noticed that I've been sneaking around using terms like \"important\" and \"critical\" to describe the properties that you want to faithfully represent in your descriptive analyses without ever defining the terms. And here's why: there are no objective definitions of these terms. What is *important* depends both (a) on the context, and (b) on the value system of you (the data scientist) and your stakeholder. \n",
    "\n",
    "To illustrate the importance of context, suppose you have data on antibiotic resistant infections in a hospital. If you were brought in by the hospital facilities director who wanted to learn what could be done to reduce infections by, say, removing fabric chairs that are hard to disinfect, the location of infections is likely important. If, by contrast, you were brought in by someone studying how the ways antibiotics were prescribed impacted infections, you would instead want to focus on the treatment histories of patients. And if you were hired by the hospital itself who just wanted to reduce infections by any means possible, you'd want to study both to know where future efforts might be best targeted. \n",
    "\n",
    "The role of values is more subtle, but no less important. \n",
    "\n",
    "Suppose you are a policy maker choosing between two possible policies for reducing $CO_2$ emmissions in the United States. You are told that Policy A would reduce $CO_2$ emmissions by 95%, have only a minimal impact on unemployment and business profits, and would require a 100 million dollar tax. You are also told Policy B would reduce $CO_2$ emmissions by only 90%, would have a medium impact on unemployment and business profits, and would require a 200 million dollar tax. Which would you choose?\n",
    "\n",
    "Now suppose I also told you that the 100 million dollar in taxes from Policy A would come entirely from taxing people who live below the poverty line, while the 200 million dollar tax for Policy B would be collected from all Americans at a fixed tax rate. Does that change how you see the issue?\n",
    "\n",
    "People tend to make decisions on the basis of the information that is available to them, and so what questions get asked (and what data is thus presented) can have a *huge* impact on how decisions are made. And as a data scientist, you will often be in control of what questions are being asked, and so it is incumbent upon you to ensure that your stakeholders are presented with all the data that *you* feel is important to know. \n",
    "\n",
    "This is actually one of the big reasons that the lack of diversity in data science is such a big problem -- not because white men are intrinsically misogynistic or racist, but because our life experiences influence what we think is important, and thus what we ask our data. \n",
    "\n",
    "Consider this infamous (though thankfully low stakes) illustrative example of a major tech failure (seriously, [go watch the video](https://www.youtube.com/watch?v=t4DT3tQqgRM)): the camera that only sees White people. We can't know exactly what went wrong, but I think it's safe to say that were there more black developers working at HP, surely *one* of them would have stopped to ask the question \"does this work as well for black faces as white faces?\" And maybe this wouldn't have shipped. \n",
    "\n",
    "OK, fine, but that was a silly webcam and HP is not exactly at the forefront of technological innovation. A more serious tech company would never make that kind of mistake. [Oh wait... they did.](https://www.theverge.com/2018/1/12/16882408/google-racist-gorillas-photo-recognition-algorithm-ai)\n",
    "\n",
    "To be clear, it's not *just* that these companies created racist algorithms -- as we'll discuss later in this class, almost any machine learning tool trained on public data will end up reflecting all the biases of our society -- it's that *they shipped the racist products!* No one in these companies thought to stop and ask the question \"hey, before we roll these out, should we check to see if they're racist?\" \n",
    "\n",
    "(And yes, these are awful but relatively low stakes examples -- later in this course we'll discuss how the real-world use of biased algorithms has resulted in [more Black people being unfairly sent to jail](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing) and [women being discriminated against in hiring](https://www.reuters.com/article/us-amazon-com-jobs-automation-insight/amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK08G).)\n",
    "\n",
    "So remember: when deciding what to look at and report in your analyses, remember that people will make decisions on the basis of the data you provide, so if you don't stop to ask a question about, say, gender or racial bias and include that in your report, odds are it won't be something considered by policy makers. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
