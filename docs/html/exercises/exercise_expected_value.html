<!DOCTYPE html> <html lang=en  data-content_root="../"> <meta charset=utf-8  /> <meta name=viewport  content="width=device-width, initial-scale=1.0" /><meta name=viewport  content="width=device-width, initial-scale=1" /> <meta name=viewport  content="width=device-width,initial-scale=1"> <meta http-equiv=x-ua-compatible  content="ie=edge"> <meta name="lang:clipboard.copy" content="Copy to clipboard"> <meta name="lang:clipboard.copied" content="Copied to clipboard"> <meta name="lang:search.language" content=en > <meta name="lang:search.pipeline.stopwords" content=True > <meta name="lang:search.pipeline.trimmer" content=True > <meta name="lang:search.result.none" content="No matching documents"> <meta name="lang:search.result.one" content="1 matching document"> <meta name="lang:search.result.other" content="# matching documents"> <meta name="lang:search.tokenizer" content="[\s\-]+"> <link href="https://fonts.gstatic.com/" rel=preconnect  crossorigin> <link href="https://fonts.googleapis.com/css?family=Roboto+Mono:400,500,700|Roboto:300,400,400i,700&display=fallback" rel=stylesheet > <style> body, input { font-family: "Roboto", "Helvetica Neue", Helvetica, Arial, sans-serif } code, kbd, pre { font-family: "Roboto Mono", "Courier New", Courier, monospace } </style> <link rel=stylesheet  href="../_static/stylesheets/application.css"/> <link rel=stylesheet  href="../_static/stylesheets/application-palette.css"/> <link rel=stylesheet  href="../_static/stylesheets/application-fixes.css"/> <link rel=stylesheet  href="../_static/fonts/material-icons.css"/> <meta name=theme-color  content="#2196f3"> <script src="../_static/javascripts/modernizr.js"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=UA-151397036-1"></script> <script> window.dataLayer = window.dataLayer || []; function gtag() { dataLayer.push(arguments); } gtag('js', new Date()); gtag('config', 'UA-151397036-1'); </script> <title>Decision Making from AB Testing &#8212; Unifying Data Science</title> <link rel=stylesheet  type="text/css" href="../_static/pygments.css?v=649a27d8" /> <link rel=stylesheet  type="text/css" href="../_static/material.css?v=79c92029" /> <link rel=stylesheet  type="text/css" href="../_static/nbsphinx-code-cells.css?v=14571329" /> <script src="../_static/documentation_options.js?v=5929fcd5"></script> <script src="../_static/doctools.js?v=888ff710"></script> <script src="../_static/sphinx_highlight.js?v=dc90522c"></script> <script crossorigin=anonymous  integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script> <script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "tex2jax_ignore|mathjax_ignore|document", "processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script> <script defer=defer  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script> <link rel=icon  href="../_static/mids_logo.svg"/> <link rel=index  title=Index  href="../genindex.html" /> <link rel=search  title=Search  href="../search.html" /> <body dir=ltr data-md-color-primary=blue-grey data-md-color-accent=blue> <svg class=md-svg > <defs data-children-count=0 > <svg xmlns="http://www.w3.org/2000/svg" width=416  height=448  viewBox="0 0 416 448" id=__github ><path fill=currentColor  d="M160 304q0 10-3.125 20.5t-10.75 19T128 352t-18.125-8.5-10.75-19T96 304t3.125-20.5 10.75-19T128 256t18.125 8.5 10.75 19T160 304zm160 0q0 10-3.125 20.5t-10.75 19T288 352t-18.125-8.5-10.75-19T256 304t3.125-20.5 10.75-19T288 256t18.125 8.5 10.75 19T320 304zm40 0q0-30-17.25-51T296 232q-10.25 0-48.75 5.25Q229.5 240 208 240t-39.25-2.75Q130.75 232 120 232q-29.5 0-46.75 21T56 304q0 22 8 38.375t20.25 25.75 30.5 15 35 7.375 37.25 1.75h42q20.5 0 37.25-1.75t35-7.375 30.5-15 20.25-25.75T360 304zm56-44q0 51.75-15.25 82.75-9.5 19.25-26.375 33.25t-35.25 21.5-42.5 11.875-42.875 5.5T212 416q-19.5 0-35.5-.75t-36.875-3.125-38.125-7.5-34.25-12.875T37 371.5t-21.5-28.75Q0 312 0 260q0-59.25 34-99-6.75-20.5-6.75-42.5 0-29 12.75-54.5 27 0 47.5 9.875t47.25 30.875Q171.5 96 212 96q37 0 70 8 26.25-20.5 46.75-30.25T376 64q12.75 25.5 12.75 54.5 0 21.75-6.75 42 34 40 34 99.5z"/></svg> </defs> </svg> <input class=md-toggle  data-md-toggle=drawer  type=checkbox  id=__drawer > <input class=md-toggle  data-md-toggle=search  type=checkbox  id=__search > <label class=md-overlay  data-md-component=overlay  for=__drawer ></label> <a href="#exercises/exercise_expected_value" tabindex=1  class=md-skip > Skip to content </a> <header class=md-header  data-md-component=header > <nav class="md-header-nav md-grid"> <div class="md-flex navheader"> <div class="md-flex__cell md-flex__cell--shrink"> <a href="../index.html" title="Unifying Data Science" class="md-header-nav__button md-logo"> &nbsp; </a> </div> <div class="md-flex__cell md-flex__cell--shrink"> <label class="md-icon md-icon--menu md-header-nav__button" for=__drawer ></label> </div> <div class="md-flex__cell md-flex__cell--stretch"> <div class="md-flex__ellipsis md-header-nav__title" data-md-component=title > <span class=md-header-nav__topic >Unifying Data Science</span> <span class=md-header-nav__topic > Decision Making from AB Testing </span> </div> </div> <div class="md-flex__cell md-flex__cell--shrink"> <label class="md-icon md-icon--search md-header-nav__button" for=__search ></label> <div class=md-search  data-md-component=search  role=dialog > <label class=md-search__overlay  for=__search ></label> <div class=md-search__inner  role=search > <form class=md-search__form  action="../search.html" method=get  name=search > <input type=text  class=md-search__input  name=q  placeholder=""Search"" autocapitalize=off  autocomplete=off  spellcheck=false  data-md-component=query  data-md-state=active > <label class="md-icon md-search__icon" for=__search ></label> <button type=reset  class="md-icon md-search__icon" data-md-component=reset  tabindex=-1 > &#xE5CD; </button> </form> <div class=md-search__output > <div class=md-search__scrollwrap  data-md-scrollfix> <div class=md-search-result  data-md-component=result > <div class=md-search-result__meta > Type to start searching </div> <ol class=md-search-result__list ></ol> </div> </div> </div> </div> </div> </div> <script src="../_static/javascripts/version_dropdown.js"></script> <script> var json_loc = "../"versions.json"", target_loc = "../../", text = "Versions"; $( document ).ready( add_version_dropdown(json_loc, target_loc, text)); </script> </div> </nav> </header> <div class=md-container > <nav class=md-tabs  data-md-component=tabs > <div class="md-tabs__inner md-grid"> <ul class=md-tabs__list > <li class=md-tabs__item ><a href="../index.html" class=md-tabs__link >Home</a> <li class=md-tabs__item ><a href="../class_schedule.html" class=md-tabs__link >Class Schedule</a> <li class=md-tabs__item ><a href="../topic_list.html" class=md-tabs__link >Topic List</a> <li class=md-tabs__item ><a href="https://www.nickeubank.com" class=md-tabs__link >About The Author</a> </ul> </div> </nav> <main class=md-main > <div class="md-main__inner md-grid" data-md-component=container > <div class="md-sidebar md-sidebar--primary" data-md-component=navigation > <div class=md-sidebar__scrollwrap > <div class=md-sidebar__inner > <nav class="md-nav md-nav--primary" data-md-level=0 > <label class="md-nav__title md-nav__title--site" for=__drawer > <a href="../index.html" title="Unifying Data Science" class="md-nav__button md-logo"> <img src="../_static/" alt=" logo" width=48  height=48 > </a> <a href="../index.html" title="Unifying Data Science">Unifying Data Science</a> </label> <ul class=md-nav__list > <li class=md-nav__item > <a href="../class_schedule.html" class=md-nav__link >CLASS SCHEDULE</a> <li class=md-nav__item > <span class="md-nav__link caption"><span class=caption-text >Causal Inference</span></span> <li class=md-nav__item > <a href="../limitations_of_ATE.html" class=md-nav__link >Limitations of ATE</a> <li class=md-nav__item > <a href="../internal_v_external_validity.html" class=md-nav__link >Internal v. External Validity</a> <li class=md-nav__item > <a href="../evaluating_real_studies.html" class=md-nav__link >Evaluating A Real Study</a> <li class=md-nav__item > <a href="../causal_inference_beyond_ab_testing.html" class=md-nav__link >Beyond AB Testing</a> <li class=md-nav__item > <a href="../matching_why.html" class=md-nav__link >Matching (Why)</a> <li class=md-nav__item > <a href="../matching_how.html" class=md-nav__link >Matching (How)</a> <li class=md-nav__item > <a href="../interpreting_indicator_vars.html" class=md-nav__link >Indicator Variables</a> <li class=md-nav__item > <a href="../fixed_effects.html" class=md-nav__link >Fixed Effects (FEs)</a> <li class=md-nav__item > <a href="../fixed_effects_and_causal_inference.html" class=md-nav__link >FEs & Causality</a> <li class=md-nav__item > <a href="../fixed_effects_v_hierarchical.html" class=md-nav__link >FEs & Hierarchical Models</a> <li class=md-nav__item > <span class="md-nav__link caption"><span class=caption-text >Data Science Project Design</span></span> <li class=md-nav__item > <a href="../backwards_design.html" class=md-nav__link >Backwards Design</a> <li class=md-nav__item > <a href="../taxonomy_of_questions.html" class=md-nav__link >Taxonomy of Questions</a> <li class=md-nav__item > <a href="../moving_from_problems_to_questions.html" class=md-nav__link >From Problems to Questions</a> <li class=md-nav__item > <a href="../descriptive_questions.html" class=md-nav__link >Discretion and Description</a> <li class=md-nav__item > <a href="../ethical_ml_recommendations.html" class=md-nav__link >Ethical Machine Learning</a> <li class=md-nav__item > <a href="../writing_to_stakeholders.html" class=md-nav__link >Writing for Lay Audiences</a> </ul> </nav> </div> </div> </div> <div class="md-sidebar md-sidebar--secondary" data-md-component=toc > <div class=md-sidebar__scrollwrap > <div class=md-sidebar__inner > <nav class="md-nav md-nav--secondary"> <label class=md-nav__title  for=__toc >"Contents"</label> <ul class=md-nav__list  data-md-scrollfix=""> <li class=md-nav__item ><a href="#exercises-exercise-expected-value--page-root" class=md-nav__link >Decision Making from AB Testing</a><nav class=md-nav > <ul class=md-nav__list > <li class=md-nav__item ><a href="#Exercise-Context" class=md-nav__link >Exercise Context</a> <li class=md-nav__item ><a href="#Exercises" class=md-nav__link >Exercises</a><nav class=md-nav > <ul class=md-nav__list > <li class=md-nav__item ><a href="#Exercise-1" class=md-nav__link >Exercise 1</a> <li class=md-nav__item ><a href="#Exercise-2" class=md-nav__link >Exercise 2</a> <li class=md-nav__item ><a href="#Exercise-3" class=md-nav__link >Exercise 3</a> <li class=md-nav__item ><a href="#Exercise-4" class=md-nav__link >Exercise 4</a> </ul> </nav> <li class=md-nav__item ><a href="#Bootstrapping" class=md-nav__link >Bootstrapping</a><nav class=md-nav > <ul class=md-nav__list > <li class=md-nav__item ><a href="#Exercise-5" class=md-nav__link >Exercise 5</a> <li class=md-nav__item ><a href="#Exercise-6" class=md-nav__link >Exercise 6</a> <li class=md-nav__item ><a href="#Exercise-7" class=md-nav__link >Exercise 7</a> <li class=md-nav__item ><a href="#Exercise-8" class=md-nav__link >Exercise 8</a> </ul> </nav> <li class=md-nav__item ><a href="#What-We-Just-Did" class=md-nav__link >What We Just Did</a><nav class=md-nav > <ul class=md-nav__list > <li class=md-nav__item ><a href="#Rejecting-the-Null" class=md-nav__link >Rejecting the Null</a> <li class=md-nav__item ><a href="#Confidence-Intervals" class=md-nav__link >Confidence Intervals</a> <li class=md-nav__item ><a href="#What-It-Means-For-This-To-Be-Frequentist" class=md-nav__link >What It Means For This To Be Frequentist</a> </ul> </nav> <li class=md-nav__item ><a href="#A-(Slightly-Heretical)-Bayesian-Perspective" class=md-nav__link >A (Slightly Heretical) Bayesian Perspective</a> <li class=md-nav__item ><a href="#Outcome-Simulation" class=md-nav__link >Outcome Simulation</a><nav class=md-nav > <ul class=md-nav__list > <li class=md-nav__item ><a href="#Exercise-9" class=md-nav__link >Exercise 9</a> <li class=md-nav__item ><a href="#You-Did-It!" class=md-nav__link >You Did It!</a> <li class=md-nav__item ><a href="#Exercise-10" class=md-nav__link >Exercise 10</a> <li class=md-nav__item ><a href="#Exercise-11" class=md-nav__link >Exercise 11</a> <li class=md-nav__item ><a href="#Exercise-12" class=md-nav__link >Exercise 12</a> <li class=md-nav__item ><a href="#Exercise-13" class=md-nav__link >Exercise 13</a> </ul> </nav> <li class=md-nav__item ><a href="#The-Math" class=md-nav__link >The Math</a><nav class=md-nav > <ul class=md-nav__list > <li class=md-nav__item ><a href="#Bayes-Rule-and-Frequentist-Statistics" class=md-nav__link >Bayes Rule and Frequentist Statistics</a> <li class=md-nav__item ><a href="#OK,-why-do-I-care?" class=md-nav__link >OK, why do I care?</a> <li class=md-nav__item ><a href="#Reason-1:-It-is-critically-important-that-you-understand-what-p-values-and-other-Frequentist-statistics-are,-and-what-they-are-not." class=md-nav__link >Reason 1: It is <em>critically</em> important that you understand what p-values and other Frequentist statistics are, and what they are not.</a> <li class=md-nav__item ><a href="#Reason-2:-There-is-a-special-case-where-P(ATE|-X)-=-P(X|-ATE)" class=md-nav__link >Reason 2: There <em>is</em> a special case where <span class="math notranslate nohighlight">\(P(ATE| X) = P(X| ATE)\)</span></a> <li class=md-nav__item ><a href="#Reason-3:-The-Bernstein-von-Mises-Theorem" class=md-nav__link >Reason 3: The Bernstein-von Mises Theorem</a> </ul> </nav> <li class=md-nav__item ><a href="#Doing-It-Like-A-Bayesian" class=md-nav__link >Doing It Like A Bayesian</a> <li class=md-nav__item ><a href="#Still-Reading?" class=md-nav__link >Still Reading?</a> </ul> </nav> </ul> </nav> </div> </div> </div> <div class=md-content > <article class="md-content__inner md-typeset" role=main > <section id=Decision-Making-from-AB-Testing > <h1 id=exercises-exercise-expected-value--page-root >Decision Making from AB Testing<a class=headerlink  href="#exercises-exercise-expected-value--page-root" title="Link to this heading">¶</a></h1> <p>Most presentations of A/B testing tend to emphasize a binary approach to decision making: we run an AB test, then evaluate whether we can reject the null hypothesis of no effect. If we reject the null hypothesis of no effect, then we deploy the change; if not, we maintain the status quo.</p> <p>We have also seen some approaches that bring a little much needed nuance to this approach: in our <a class="reference external" href="https://www.amazon.com/Trustworthy-Online-Controlled-Experiments-Practical/dp/1108724264">Trustworthy Online Controlled Experiments</a>, the authors have emphasized that we should evaluate <em>both</em></p> <ul class=simple > <li><p>Can we reject the null hypothesis of no effect (to establish our results are statistically significant), and</p> <li><p>Whether the estimated effect is of <em>practical significance</em>, by which they mean “does the effect size look large enough that it would be profitable to deploy.”</p> </ul> <p>And finally, you’ve probably seen cases that do the thing you should probably be doing (at least within this regime):</p> <ul class=simple > <li><p>Rather than evaluating statistical significance with respect to a null hypothesis of no effect, evaluate statistical significance with respect to a null hypothesis of “the effect is less than the size needed for deployment to be profitable (i.e., less than or equal to the threshold for practical significance).”</p> </ul> <p>In these exercises, we will examine an approach to analyzing the results of an A/B test that is a little less black and white, and I think provides a more holistic, easy to understand, and sophisticated way of interpreting A/B test results. It comes with a couple caveats, but I’d argue none that don’t apply to the approaches described above.</p> <p>In short, we will:</p> <ul class=simple > <li><p>Analyze our data using the Frequentist statistical methods you are familiar with. Rather than just looking at p-values and rejecting or not rejecting a null hypothesis, however, we will instead interpret our results as probability distributions for what effect we might expect our experiment to have if we were to deploy it more broadly.</p> <li><p>We will use this probability distribution as the basis for some Monte Carlo simulation-based analyses of the likelihood of different outcomes.</p> <li><p>Finally, we will touch on how this way of thinking about the result of our analyses relates to the Bayesian approach to statistical analysis.</p> </ul> <p>That probably sounds pretty abstract, so to make it all more concrete, let’s just start by doing our usual type of analysis of an A/B test.</p> <section id=Exercise-Context > <h2 id=Exercise-Context >Exercise Context<a class=headerlink  href="#Exercise-Context" title="Link to this heading">¶</a></h2> <p>In these exercises, we will be working with a tweaked version of data from <a class="reference external" href="https://www.kaggle.com/datasets/chebotinaa/fast-food-marketing-campaign-ab-test">IBM Watson Analytics from a marketing A/B test conducted in the state of Washington.</a></p> <p>The context in which this data was generated is:</p> <blockquote> <div><p>A fast-food chain plans to add a new item to its menu. However, they are still undecided whether to run a new marketing campaign for promoting the new product. In order to determine whether the promotion will increase sales, a promotion has been deployed at a random sample of stores to promote the new item. Weekly sales of the new item are recorded for the first four weeks.</p> </div></blockquote> <p>The data consists of the following variables:</p> <ul class=simple > <li><p><code class="docutils literal notranslate"><span class=pre >MarketID</span></code>: unique identifier for market</p> <li><p><code class="docutils literal notranslate"><span class=pre >MarketSize</span></code>: size of market area by sales</p> <li><p><code class="docutils literal notranslate"><span class=pre >LocationID</span></code>: unique identifier for store location</p> <li><p><code class="docutils literal notranslate"><span class=pre >AgeOfStore</span></code>: age of store in years</p> <li><p><code class="docutils literal notranslate"><span class=pre >Promotion</span></code>: did the location receive the promotion (was it treated).</p> <li><p><code class="docutils literal notranslate"><span class=pre >week</span></code>: one of four weeks when the promotions were run</p> <li><p><code class="docutils literal notranslate"><span class=pre >SalesInThousands</span></code>: sales amount for a specific <code class="docutils literal notranslate"><span class=pre >LocationID</span></code>, <code class="docutils literal notranslate"><span class=pre >Promotion</span></code>, and <code class="docutils literal notranslate"><span class=pre >week</span></code></p> </ul> <p>(And yes, despite my general distaste for them, this is a pretty darn clean dataset. You’re welcome. :) )</p> </section> <section id=Exercises > <h2 id=Exercises >Exercises<a class=headerlink  href="#Exercises" title="Link to this heading">¶</a></h2> <section id=Exercise-1 > <h3 id=Exercise-1 >Exercise 1<a class=headerlink  href="#Exercise-1" title="Link to this heading">¶</a></h3> <p>Load the dataset — <code class="docutils literal notranslate"><span class=pre >WA_Marketing_Campaign.csv</span></code> from <a class="reference external" href="https://github.com/nickeubank/MIDS_Data/tree/master/fast_food_ab_test">this repository.</a> Please use this copy (not data from the Kaggle competition from which it was drawn) as I’ve made a number of modifications to the data for this exercise.</p> </section> <section id=Exercise-2 > <h3 id=Exercise-2 >Exercise 2<a class=headerlink  href="#Exercise-2" title="Link to this heading">¶</a></h3> <p>This data is currently structured as a “panel dataset”, meaning that there are multiple observations for each store, one per week for four weeks. To simplify our analysis, let’s sum up sales from across all four weeks for each store so we only have one observation per store.</p> <p>(In a dataset where multiple observations come from the same entity at different points in time, a proper analysis requires accounting for the fact that the different observations for each entity are not independent of one another using a method like clustering of our standard errors.)</p> <p>You should end up with 90 observations (one per <code class="docutils literal notranslate"><span class=pre >LocationID</span></code>, the identifier for individual stores).</p> </section> <section id=Exercise-3 > <h3 id=Exercise-3 >Exercise 3<a class=headerlink  href="#Exercise-3" title="Link to this heading">¶</a></h3> <p>Now, using a simple linear regression in <code class="docutils literal notranslate"><span class=pre >statsmodels</span></code>, estimate the simple difference in means between stores with and without the promotion.</p> <p>(We’re skipping a lot of “learn about your data” and “validate your randomization by checking balance” steps — this is bad practice, but again I want us to stay focused on the analysis and interpretation for this exercise, so you can just trust it’s ok.)</p> <p>Based on this simple regression, would you reject the null hypothesis that the experiment had no effect using a p-value threshold of 0.05?</p> </section> <section id=Exercise-4 > <h3 id=Exercise-4 >Exercise 4<a class=headerlink  href="#Exercise-4" title="Link to this heading">¶</a></h3> <p>We have a lot of important controls in this dataset, so add in categorical controls for <code class="docutils literal notranslate"><span class=pre >MarketID</span></code> and <code class="docutils literal notranslate"><span class=pre >log(AgeOfStore)</span></code>.</p> <p>Now would you feel comfortable rejecting the null hypothesis of no effect at a p-value threshold of 0.05?</p> </section> </section> <section id=Bootstrapping > <h2 id=Bootstrapping >Bootstrapping<a class=headerlink  href="#Bootstrapping" title="Link to this heading">¶</a></h2> <p>When we run a linear regression — like the one above — standard errors are calculated under the assumption that certain assumptions about our data are true. Generally speaking, those are that our regression errors are normally distributed and homoskedastic.</p> <p>These analytically-derived standard errors are not the only way one can calculate standard errors, however. A different approach — and one whose validity does not rest on any distributional assumptions: bootstrapping.</p> <p>Within Frequentist statistics, the meaning of standard errors is “how would our estimates vary if we conducted our study all over again, drawing a new set of observations from the same population each time?”</p> <p>Bootstrapping takes this idea seriously. Essentially bootstrapping is the process of simulating drawing new observations from the larger population and observing how our estimate varies across different draws of data. In doing so, bootstrapping only relies on the idea that our data was randomly drawn from a larger population, not any distributional assumptions. In most cases, that makes it much more robust.</p> <p>OK, but obviously we can’t go re-run this marketing experiment again, so how do we get more data? It turns out that sampling from our actual data <em>with replacement</em> to get new datasets of the same size as our original dataset is a statistically valid way to simulate re-running the experiment.</p> <p>(I know I’ve hinted at the fact we’ll be talking about Bayesian inference later, but to be clear, this is not Bayesian — this is just a different way of calculating standard errors within Frequentist statistics that doesn’t require the distributional assumptions of the standard errors we get from a normal regression).</p> <section id=Exercise-5 > <h3 id=Exercise-5 >Exercise 5<a class=headerlink  href="#Exercise-5" title="Link to this heading">¶</a></h3> <p>To create a bootstrapped estimate of the difference in means between treatment and control, create a loop that runs 10,000 times, and at each step:</p> <ol class="arabic simple"> <li><p>Creates a new dataset by sampling — with replacement — from our original dataset. This new dataset should be the same size as our original dataset.</p> <li><p>Runs the regression we specified above (total sales over four weeks regressed on <code class="docutils literal notranslate"><span class=pre >Promotion</span></code>, <code class="docutils literal notranslate"><span class=pre >MarketID</span></code>, and <code class="docutils literal notranslate"><span class=pre >log(AgeOfStore)</span></code>).</p> <li><p>Extracts the coefficient on <code class="docutils literal notranslate"><span class=pre >Promotion</span></code> from said regression and stores it in a new series or numpy array.</p> </ol> <p>As usual with loops, don’t try and write the final loop all at once — put together one pass, then put it in a loop that runs a few times, then finally a loop that runs all 100,000 times.</p> <p>(Note: the reason we collapsed our data to one-observation-per-LocationID is that if we’d still had multiple weeks per store, we’d have to re-sample our data a little differently. In particular, we’d have to randomly draw from a list of stores, then pull all the observations per store to simulate “drawing” new <em>stores</em> with replacement, rather than drawing observations with replacement.)</p> </section> <section id=Exercise-6 > <h3 id=Exercise-6 >Exercise 6<a class=headerlink  href="#Exercise-6" title="Link to this heading">¶</a></h3> <p>What is the average value of your bootstrapped estimate? What is the standard deviation? Finally, plot a histogram of your estimates.</p> <p>If it’s helpful, recall that pandas Series have a <code class="docutils literal notranslate"><span class=pre >.hist()</span></code> method for this purpose. I’d also recommend using the <code class="docutils literal notranslate"><span class=pre >bins</span></code> keyword if it’s too lumpy.</p> <p>How does the distribution of these bootstrapped estimates compare with the coefficient estimate (and its standard error) from your regression.</p> </section> <section id=Exercise-7 > <h3 id=Exercise-7 >Exercise 7<a class=headerlink  href="#Exercise-7" title="Link to this heading">¶</a></h3> <p>While there is a way to get p-values from bootstrapping, it actually requires a different kind of bootstrapping we don’t want/need to get into here.</p> <p>But what we <em>can</em> ask is “if we did this experiment again, what is the likelihood that we’d get an estimate of the effect of <code class="docutils literal notranslate"><span class=pre >Promotion</span></code> that is 0 or less?” We do this by looking to see if 0 is within the 95% confidence interval of our estimate.</p> <p>This we can compute by sorting our estimates and looking at the bottom 2.5% (since confidence intervals are two-tailed, the 95% confidence interval excludes 2.5% of the probability mass on both sides). If that cutoff is above 0, we can say that 0 is outside the 95% confidence interval of our estimate.</p> <p>What is the lower 2.5% cutoff?</p> </section> <section id=Exercise-8 > <h3 id=Exercise-8 >Exercise 8<a class=headerlink  href="#Exercise-8" title="Link to this heading">¶</a></h3> <p>Given the answer you got to Exercise 5 (very similar mean and standard deviation to what you got from the analytically calculated standard errors), you may be wondering why we bothered bootstrapping. And you wouldn’t be wrong — as it turns out, the errors in the vanilla linear regression we ran above are quite normally distributed, and as a result, the standard errors are also accurate.</p> <p>To illustrate, here’s a Q-Q plot of the regression residuals. <a class="reference external" href="https://en.wikipedia.org/wiki/Q%E2%80%93Q_plot">Q-Q plot</a> plots the quantiles of data against quantiles of a perfect distribution (here a normal distribution). The closer the data is to the expected line, the closer the data is to being distributed according to the specified distribution.</p> <div class="nbinput docutils container"> <div class="prompt highlight-none notranslate"><div class=highlight ><pre><span></span>[11]:
</pre></div> </div> <div class="input_area highlight-ipython3 notranslate"><div class=highlight ><pre><span></span><span class=kn >from</span> <span class=nn >statsmodels.graphics.gofplots</span> <span class=kn >import</span> <span class=n >qqplot</span>

<span class=n >qqplot</span><span class=p >(</span><span class=n >fit_model</span><span class=o >.</span><span class=n >resid</span><span class=p >,</span> <span class=n >line</span><span class=o >=</span><span class=s2 >"s"</span><span class=p >)</span>
</pre></div> </div> </div> <div class="nboutput docutils container"> <div class="prompt highlight-none notranslate"><div class=highlight ><pre><span></span>[11]:
</pre></div> </div> <div class="output_area docutils container"> <img alt="../_images/exercises_exercise_expected_value_10_0.png" src="../_images/exercises_exercise_expected_value_10_0.png"/> </div> </div> <div class="nboutput nblast docutils container"> <div class="prompt empty docutils container"> </div> <div class="output_area docutils container"> <img alt="../_images/exercises_exercise_expected_value_10_1.png" src="../_images/exercises_exercise_expected_value_10_1.png"/> </div> </div> <p>And if we overlay data sampled from a normal distribution with the standard error given by the regression, we’d also get a distribution similar to that generated by our bootstrap:</p> <div class="nbinput docutils container"> <div class="prompt highlight-none notranslate"><div class=highlight ><pre><span></span>[12]:
</pre></div> </div> <div class="input_area highlight-ipython3 notranslate"><div class=highlight ><pre><span></span><span class=kn >import</span> <span class=nn >seaborn.objects</span> <span class=k >as</span> <span class=nn >so</span>
<span class=kn >from</span> <span class=nn >matplotlib</span> <span class=kn >import</span> <span class=n >style</span>
<span class=kn >import</span> <span class=nn >matplotlib.pyplot</span> <span class=k >as</span> <span class=nn >plt</span>
<span class=kn >import</span> <span class=nn >warnings</span>

<span class=n >warnings</span><span class=o >.</span><span class=n >simplefilter</span><span class=p >(</span><span class=n >action</span><span class=o >=</span><span class=s2 >"ignore"</span><span class=p >,</span> <span class=n >category</span><span class=o >=</span><span class=ne >FutureWarning</span><span class=p >)</span>

<span class=c1 ># Get normal draws according to analytic regression</span>
<span class=c1 ># standard errors</span>
<span class=kn >import</span> <span class=nn >numpy.random</span> <span class=k >as</span> <span class=nn >npr</span>

<span class=n >normal_draws</span> <span class=o >=</span> <span class=n >npr</span><span class=o >.</span><span class=n >normal</span><span class=p >(</span>
    <span class=n >loc</span><span class=o >=</span><span class=n >fit_model</span><span class=o >.</span><span class=n >params</span><span class=p >[</span><span class=s2 >"Promotion"</span><span class=p >],</span> <span class=n >scale</span><span class=o >=</span><span class=n >fit_model</span><span class=o >.</span><span class=n >bse</span><span class=p >[</span><span class=s2 >"Promotion"</span><span class=p >],</span> <span class=n >size</span><span class=o >=</span><span class=mi >100_000</span>
<span class=p >)</span>
<span class=n >normals</span> <span class=o >=</span> <span class=n >pd</span><span class=o >.</span><span class=n >DataFrame</span><span class=p >({</span><span class=s2 >"estimates"</span><span class=p >:</span> <span class=n >normal_draws</span><span class=p >,</span> <span class=s2 >"source"</span><span class=p >:</span> <span class=s2 >"Normal Draws"</span><span class=p >})</span>

<span class=c1 ># Combine with bootstrap estimates for ease of plotting</span>
<span class=n >boot</span> <span class=o >=</span> <span class=n >pd</span><span class=o >.</span><span class=n >DataFrame</span><span class=p >({</span><span class=s2 >"estimates"</span><span class=p >:</span> <span class=n >estimates</span><span class=p >,</span> <span class=s2 >"source"</span><span class=p >:</span> <span class=s2 >"Bootstrap"</span><span class=p >})</span>
<span class=n >normal_and_boot</span> <span class=o >=</span> <span class=n >pd</span><span class=o >.</span><span class=n >concat</span><span class=p >([</span><span class=n >normals</span><span class=p >,</span> <span class=n >boot</span><span class=p >])</span>

<span class=p >(</span>
    <span class=n >so</span><span class=o >.</span><span class=n >Plot</span><span class=p >(</span><span class=n >normal_and_boot</span><span class=p >,</span> <span class=n >x</span><span class=o >=</span><span class=s2 >"estimates"</span><span class=p >,</span> <span class=n >color</span><span class=o >=</span><span class=s2 >"source"</span><span class=p >)</span>
    <span class=o >.</span><span class=n >add</span><span class=p >(</span><span class=n >so</span><span class=o >.</span><span class=n >Bars</span><span class=p >(),</span> <span class=n >so</span><span class=o >.</span><span class=n >Hist</span><span class=p >(</span><span class=n >stat</span><span class=o >=</span><span class=s2 >"density"</span><span class=p >,</span> <span class=n >common_bins</span><span class=o >=</span><span class=kc >True</span><span class=p >,</span> <span class=n >common_norm</span><span class=o >=</span><span class=kc >True</span><span class=p >))</span>
    <span class=o >.</span><span class=n >label</span><span class=p >(</span><span class=n >title</span><span class=o >=</span><span class=s2 >"Normal Distribution Draws v. Bootstrap"</span><span class=p >)</span>
    <span class=o >.</span><span class=n >theme</span><span class=p >({</span><span class=o >**</span><span class=n >style</span><span class=o >.</span><span class=n >library</span><span class=p >[</span><span class=s2 >"seaborn-v0_8-whitegrid"</span><span class=p >]})</span>
<span class=p >)</span>
</pre></div> </div> </div> <div class="nboutput nblast docutils container"> <div class="prompt highlight-none notranslate"><div class=highlight ><pre><span></span>[12]:
</pre></div> </div> <div class="output_area docutils container"> <img alt="../_images/exercises_exercise_expected_value_12_0.png" class=no-scaled-link  src="../_images/exercises_exercise_expected_value_12_0.png" style="width: 643.4499999999999px; height: 378.25px;"/> </div> </div> <p>But as well see very soon, in addition to being more flexible in terms of being able to accommodate data that is not normally distributed, we’re going to make use of the array of sampled estimates bootstrapping generated in just a moment…</p> </section> </section> <section id=What-We-Just-Did > <h2 id=What-We-Just-Did >What We Just Did<a class=headerlink  href="#What-We-Just-Did" title="Link to this heading">¶</a></h2> <p>Let’s talk a little about the different quantities we’ve calculated and how we can interpret them.</p> <section id=Rejecting-the-Null > <h3 id=Rejecting-the-Null >Rejecting the Null<a class=headerlink  href="#Rejecting-the-Null" title="Link to this heading">¶</a></h3> <p>The first thing we did was run a regular linear regression. That gave us a p-value, which is the probability that we would observe a difference between our treatment and control groups (i.e., a value of the coefficient on <code class="docutils literal notranslate"><span class=pre >Promotion</span></code>) at least as large as it was <em>if the null hypothesis that ``Promotion`` had no effect</em>. In the simplest approach to frequentist statistics, that’s kinda where we start.</p> <p>A more sophisticated Frequentist would:</p> <ol class="arabic simple"> <li><p>make sure that the errors in that regression were pretty normally distributed and homoskedastic (since that’s required for our regular linear regression coefficients and standard errors to be correct),</p> <li><p>evaluate if the magnitude of the coefficient suggested it was <em>practically significant</em> (big enough that, if that were the true effect of the Promotion, it would be economically worthwhile to roll out the Promotion).</p> </ol> <p>And perhaps an especially diligent Frequentist would go one step further and specify a different null hypothesis — namely, that the Promotion’s effect was less than or equal to the practical significance threshold. Then they could get a p-value that would tell them “how likely it is we would see a coefficient at least this big if the true effect were [our threshold for practical significance] or less?” This we could do with a post-regression test (<code class="docutils literal notranslate"><span class=pre >result.t_test("Promotion</span> <span class=pre >-</span> <span class=pre >15</span> <span class=pre >=</span> <span class=pre >0")</span></code> if <code class="docutils literal notranslate"><span class=pre >15</span></code> were the threshold for practical significance).</p> </section> <section id=Confidence-Intervals > <h3 id=Confidence-Intervals >Confidence Intervals<a class=headerlink  href="#Confidence-Intervals" title="Link to this heading">¶</a></h3> <p>The second thing we did was to estimate the effect of <code class="docutils literal notranslate"><span class=pre >Promotion</span></code> using bootstrapping. This is also a Frequentist method, but one that doesn’t require we make any assumptions about the distribution of the data (i.e., we don’t have to assume the regression errors are normally distributed or homoskedastic). This approach doesn’t lend itself to p-values (there is a way to do something similar to bootstrapping to get p-values), but it gives us confidence intervals we can use similarly. These tell us that if we did this experiment over and over, we think the likelihood we’d get a coefficient on <code class="docutils literal notranslate"><span class=pre >Promotion</span></code> equal to or less than zero is definitely less than 0.05, which is kinda analogous to (though not the same as) a p-value.</p> <p>(Note that in this case, the errors from the regression <em>were</em> pretty close to being normally distributed and homoskedastic, which is why our bootstrapped distribution and the results from <code class="docutils literal notranslate"><span class=pre >statsmodels</span></code> looked very similar).</p> </section> <section id=What-It-Means-For-This-To-Be-Frequentist > <h3 id=What-It-Means-For-This-To-Be-Frequentist >What It Means For This To Be Frequentist<a class=headerlink  href="#What-It-Means-For-This-To-Be-Frequentist" title="Link to this heading">¶</a></h3> <p>Everything we’ve done up until now, as I mentioned, we’ve done using traditional tools of Frequentist statistics, and we’ve been careful to interpret the results the way a Frequentist would: as an effort to characterize the uncertainty of our <em>estimate</em> of the true effect of Promotion.</p> <p>That’s because in Frequentist statistics, uncertainty comes from the fact that while there is a “True” effect of Promotion that gives rise to the data we observe, when we pull a finite set of N observations from this True Population, those N observations may not be perfectly representative of the population as a whole. Thus, we have uncertainty about whether the quantity we estimate is an accurate reflection of the “True” parameter value that underlies the data generating process that gives rise to all real data.</p> <p>(You’ll notice I’m using a lot of scare-quotes and weird capitalizations: that’s because a key assumption of Frequentist statistics is that there <em>is</em> a True Effect, and a True Population. But… those are kinda made-up?)</p> <p>So how else can we think about this?</p> </section> </section> <section id="A-(Slightly-Heretical)-Bayesian-Perspective"> <h2 id="A-(Slightly-Heretical)-Bayesian-Perspective">A (Slightly Heretical) Bayesian Perspective<a class=headerlink  href="#A-(Slightly-Heretical)-Bayesian-Perspective" title="Link to this heading">¶</a></h2> <p>Bayesians see things differently. Rather than trying to estimate the value of the <em>One True Parameter</em>, Bayesians think that all knowledge of the world is probabilistic, and everything is uncertain to one degree or another. Bayesian inference, therefore, is all about trying to think carefully about this uncertainty, and crucially to understand how we should update our beliefs about the world when we get new information (e.g., data).</p> <p>A Bayesian analysis begins with the analyst articulating their current beliefs about the likely values of the parameter of interest (their “prior beliefs,” often just referred to as “priors”). Then they use Bayes Rule to update those beliefs using the data being analyzed. This then generates a new, updated set of beliefs — called “posterior beliefs” or “posteriors” — which embody the analysts new, updated beliefs about the likely values of the parameter of interest.</p> <p>As a result, they don’t talk about the distribution of our <em>estimate</em> of the parameter, they talk about the probability distribution of the value of the parameter of interest itself.</p> <p>And now the part that’s a little heretical: one way to think about the “empirical distribution of <em>our estimate</em> of the effect of Promotion” is as our best guess for the likely distribution of our best guess for the value of the effect of Promotion.</p> <p>Or, to be more precise (and to ensure Bayesian’s don’t get <em>too</em> mad at me), if our dataset it’s too small, we can interpret that distribution as telling us:</p> <p><strong>In a world where we know nothing about the promotion we’re studying except the results of the statistical analysis, this is also our best guess for the probability distribution of the true effect of the Promotion.</strong></p> <p>Now, to be clear, the caveat I included above — that this is only true <strong>in a world where we know nothing about the likely effect of the promotion except the results of the statistical analysis</strong> — is a big one. In the language of Bayesian statistics, saying we know nothing except the results of this analysis is analogous to saying we are starting with flat (also sometimes called “uninformative”) priors.</p> <p>And that’s not a small thing — for example, the company that launched this campaign presumably at least <em>suspects</em> the promotion will improve sales. And based on its experience with past promotions, it is also probably pretty confident of the <em>approximate</em> magnitude of the biggest effect it may detect. For example, they probably know it won’t 10x sales. And if we were doing a proper Bayesian analysis, we would take that kind of information into account by introducing what are called “weakly informative priors.” (For a more detailed discussion with equations, hang on till the end of this exercise.)</p> <p>But the distribution of our estimates of the effect of the promotion we got above is precisely the same distribution a Bayesian would get if they used purely Bayesian statistics and methods, provided they started with flat priors. Indeed, I’ll demonstrate that at the end of this reading.</p> <p>I bring this up for two reasons.</p> <p>First, I think there is merit in understanding this place where Frequentist and Bayesian statistics intersect. They are founded on different philosophies about the source of uncertainty, and in more sophisticated applications they diverge substantially. But they are also not entirely alien to one another.</p> <p>But second, as I’ve mentioned before, I don’t think it’s the end of the world to sometimes use Frequentist statistical machinery and still reflect on it a little like a “poor man’s Bayesian estimate.” Yes, a fully Bayesian empirical analysis would likely generate somewhat different results, but unless you are using strong priors (asserting you have a significant sense of the likely effect of the promotion), the results you get will be relatively similar to what we are getting here.</p> <p>And if we decide that, as applied analysts, we’re ok with thinking about that distribution as a probability of treatment effects we’re likely to experience if we rolled out our experiment, we can do some really fun things.</p> </section> <section id=Outcome-Simulation > <h2 id=Outcome-Simulation >Outcome Simulation<a class=headerlink  href="#Outcome-Simulation" title="Link to this heading">¶</a></h2> <p>Interpreting the result above as the probability distribution of the effect of our promotion is only interesting if we find a way to do something with it. So let’s have some fun using Monte Carlos simulation techniques.</p> <p>Suppose we chose to deploy our promotion — what is likely to happen? We know that the most likely outcome is that the promotion will improve sales by our estimated ATE, but what are the range of possible outcomes? Even if we expect the promotion will <em>probably</em> make money, maybe what you’re really worried about is loss-avoidance, in which case what you want to know is actually “what are the odds we <em>lose</em> money?” And how can we integrate information about the cost economics of the promotion into how we answer those questions? Let’s find out!</p> <section id=Exercise-9 > <h3 id=Exercise-9 >Exercise 9<a class=headerlink  href="#Exercise-9" title="Link to this heading">¶</a></h3> <p>Let’s start with a simple example. Suppose that our promotion has a fixed cost per store per month. More specifically, suppose that it costs 17,000 dollars per store per month to run the promotion. What is the probability that, if the fast food company deployed the promotion, it would actually <em>lose</em> money?</p> <p>To answer this question, simply figure out how to compute the company’s per-store profit as a function of the ATE of the promotion.</p> <p>Then, for each estimate of the ATE we bootstrapped, calculate the profit if that estimate turned out to be the true ATE.</p> <p>Then calculate the share of bootstrapped “worlds” in which the per-store profits from the promotion were negative.</p> </section> <section id="You-Did-It!"> <h3 id="You-Did-It!">You Did It!<a class=headerlink  href="#You-Did-It!" title="Link to this heading">¶</a></h3> <p>Congratulations! You just did your first “scenario modelling!” And now you can probably see one of the reasons I chose to bootstrap our standard errors — it naturally generated this array of simulated draws of the Average Treatment Effect we can use for modelling!</p> <p>That isn’t to say bootstrapping is strictly necessary to do this type of analysis — because our data was relatively normally distributed, we could have done our scenario planning by using the <code class="docutils literal notranslate"><span class=pre >numpy.random.normal()</span></code> function to simulate draws from the normal distribution implied by our normal <code class="docutils literal notranslate"><span class=pre >statsmodels</span></code> results. But why add that step when?</p> </section> <section id=Exercise-10 > <h3 id=Exercise-10 >Exercise 10<a class=headerlink  href="#Exercise-10" title="Link to this heading">¶</a></h3> <p>OK, that wasn’t a particularly complicated scenario. Let’s suppose, instead, that the promotion costs $17,000 more per store per month, <em>and</em> 50 dollars per 1,000 in sales.</p> <p>Now what’s the probability you make LESS money with the promotion than without?</p> </section> <section id=Exercise-11 > <h3 id=Exercise-11 >Exercise 11<a class=headerlink  href="#Exercise-11" title="Link to this heading">¶</a></h3> <p>Now suppose that the firm that ran the Promotion is so confident in their promotional strategy that they approach you with an unusual pricing model. <em>They</em> think that the promotion will drive so much more sales than 18,000 per store that they say:</p> <ul class=simple > <li><p>You don’t have to pay them <em>anything</em> for the first $18,000 increase in sales per store.</p> <li><p>Beyond those first 18,000 units sold:</p> <ul> <li><p>For every 1,000 in sales beyond 18,000, what you pay us will increase <em>non-linearly</em> — namel, you’ll have to pay 10 dollars per <span class="math notranslate nohighlight">\(1,000^{1.02}\)</span> in increased sales.</p> </ul> </ul> <p>What are the odds you’ll lose money under this deal?</p> <p>NOTE: Because we’re using exponents, make sure to convert your units from “sales in 1,000s” to total sales by multiplying by 1,000.</p> </section> <section id=Exercise-12 > <h3 id=Exercise-12 >Exercise 12<a class=headerlink  href="#Exercise-12" title="Link to this heading">¶</a></h3> <p>Can you show your boss the full probability distribution of potential outcomes? Use 100 bins to ensure you get good granularity in the tails.</p> </section> <section id=Exercise-13 > <h3 id=Exercise-13 >Exercise 13<a class=headerlink  href="#Exercise-13" title="Link to this heading">¶</a></h3> <p>Fine, your boss is boring; she just wants to know the Expected Monetary Value of the promotion (recall the Expected Monetary Value of a decision under uncertainty is just <span class="math notranslate nohighlight">\(\sum_{i \in I} p_i v_i\)</span> where <span class="math notranslate nohighlight">\(p_i\)</span> is the probability of outcome <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(v_i\)</span> is the monetary value of outcome <span class="math notranslate nohighlight">\(i\)</span>). What would that be?</p> </section> </section> <section id=The-Math > <h2 id=The-Math >The Math<a class=headerlink  href="#The-Math" title="Link to this heading">¶</a></h2> <p>As I promised above, let’s take a moment to review why we can think of this distribution as being equal to the probability distribution of our true ATE <em>if and only if</em> we are willing to assume we know nothing other than the results of this study.</p> <section id=Bayes-Rule-and-Frequentist-Statistics > <h3 id=Bayes-Rule-and-Frequentist-Statistics >Bayes Rule and Frequentist Statistics<a class=headerlink  href="#Bayes-Rule-and-Frequentist-Statistics" title="Link to this heading">¶</a></h3> <p>As discussed previously, one of the challenges with the standard, Frequentist approach to A/B testing is that the statistics we get from these analyses (like p-values) rarely correspond to the substantive quantities we care about most.</p> <p>In statistical notation, our estimate of the distribution of our estimate of the Average Treatment Effect can be written: <span class="math notranslate nohighlight">\(P(X|ATE)\)</span> where <span class="math notranslate nohighlight">\(X\)</span> is the data generated by our A/B test conditional on the <span class="math notranslate nohighlight">\(ATE\)</span>.</p> <p>We sometimes fool ourselves into thinking that this quantity corresponds precisely to what we care about: the probability distribution of the true ATE given our estimate <span class="math notranslate nohighlight">\(P(ATE|X)\)</span>.</p> <p>But as you can see, <span class="math notranslate nohighlight">\(P(X|ATE) \neq P(ATE|X)\)</span>.</p> <p>Thankfully, though, Bayes Rule does provide us with a way of determining how these quantities relate to each other. Bayes rule is often written as:</p> <div class="math notranslate nohighlight"> \[P(A | B) = \frac{P(A \cap B)}{P(B)}\]</div> <p>We can rewrite <span class="math notranslate nohighlight">\(P(A \cap B)\)</span> as <span class="math notranslate nohighlight">\(P(B | A) * P(A)\)</span>, let <span class="math notranslate nohighlight">\(A\)</span> be <span class="math notranslate nohighlight">\(ATE\)</span> and <span class="math notranslate nohighlight">\(B\)</span> be our data <span class="math notranslate nohighlight">\(X\)</span> to help us understand our problem better:</p> <div class="math notranslate nohighlight"> \[P(ATE | X) = \frac{P(X|ATE) * P(ATE)}{P(X)}\]</div> <p>Here, we can see that quantity we are interested in (the probability distribution of <span class="math notranslate nohighlight">\(ATE\)</span>) on the left-hand side of the equation. We can also see that the first term on the right-hand side of the equation (<span class="math notranslate nohighlight">\(P(X|ATE)\)</span>) is the distribution we got from bootstrapping (or regular regression packages, conditional on some distributional assumptions). This leaves us with only two terms we need to understand: <span class="math notranslate nohighlight">\(P(ATE)\)</span> and <span class="math notranslate nohighlight">\(P(X)\)</span>.</p> <p>We call <span class="math notranslate nohighlight">\(P(ATE)\)</span> our unconditional <em>prior</em> belief about <span class="math notranslate nohighlight">\(ATE\)</span> — unconditional because we aren’t conditioning on <span class="math notranslate nohighlight">\(X\)</span> (the results of our analysis). This, in other words, what values of <span class="math notranslate nohighlight">\(ATE\)</span> were plausible before your analysis began.</p> <p>And <span class="math notranslate nohighlight">\(P(X)\)</span>? This term is actually kinda annoying and not that interesting, so we generally ignore it. Because we know that the left-hand side of our equation is a probability distribution, we know that the right-hand side has to integrate out to 1 (with respect to all possible values of <span class="math notranslate nohighlight">\(A\)</span>, which in this case is just whether the conditional probability distribution of <span class="math notranslate nohighlight">\(ATE\)</span>). So rather than trying to compute <span class="math notranslate nohighlight">\(P(X)\)</span> directly, we usually figure out what it <em>must be</em> indirectly by figuring out what normalization gives us a valid probability distribution.</p> <p>As a result, we often just say that the left-hand side of the equation is <em>proportional</em> to the right-hand side, and write this using the <span class="math notranslate nohighlight">\(\propto\)</span> symbol:</p> <div class="math notranslate nohighlight"> \[P(ATE | X) \propto P(X|ATE) * P(ATE )\]</div> </section> <section id="OK,-why-do-I-care?"> <h3 id="OK,-why-do-I-care?">OK, why do I care?<a class=headerlink  href="#OK,-why-do-I-care?" title="Link to this heading">¶</a></h3> <p>OK, that was a lot of math. Why do I care about all this?</p> <p>Two reasons:</p> </section> <section id="Reason-1:-It-is-critically-important-that-you-understand-what-p-values-and-other-Frequentist-statistics-are,-and-what-they-are-not."> <h3 id="Reason-1:-It-is-critically-important-that-you-understand-what-p-values-and-other-Frequentist-statistics-are,-and-what-they-are-not.">Reason 1: It is <em>critically</em> important that you understand what p-values and other Frequentist statistics are, and what they are not.<a class=headerlink  href="#Reason-1:-It-is-critically-important-that-you-understand-what-p-values-and-other-Frequentist-statistics-are,-and-what-they-are-not." title="Link to this heading">¶</a></h3> <p>P-values, as we discussed previously, are the probability of observing our data given the null hypothesis is true — i.e, <span class="math notranslate nohighlight">\(P(X|Null)\)</span>. It is <em>not</em> the probability that the null is true given the data (<span class="math notranslate nohighlight">\(P(Null|X)\)</span>).</p> <p>If you get a p-value of 0.05 from an AB test showing that <em>increasing</em> latency increases user retention on a website, you should not assume that “Oh, well this only had a 5% probability of happening by chance! Latency must be increasing retention!” Rather, you should say “um, I have a pretty strong sense (a prior, <span class="math notranslate nohighlight">\(P(Null)\)</span>) that increasing latency does <em>not</em> increase retention. So I’m gonna be much more skeptical of that result than an AB test that shows a <em>decrease</em> in latency increases user retention with a p-value of 0.05.</p> </section> <section id="Reason-2:-There-is-a-special-case-where-P(ATE|-X)-=-P(X|-ATE)"> <h3 id="Reason-2:-There-is-a-special-case-where-P(ATE|-X)-=-P(X|-ATE)">Reason 2: There <em>is</em> a special case where <span class="math notranslate nohighlight">\(P(ATE| X) = P(X| ATE)\)</span><a class=headerlink  href="#Reason-2:-There-is-a-special-case-where-P(ATE|-X)-=-P(X|-ATE)" title="Link to this heading">¶</a></h3> <p>Suppose you know <em>nothing</em> about the treatment you seek to test. All possible outcomes, in your mind, are <em>equally</em> likely. This is the case of what is called a flat prior (also sometimes called an <em>uninformative prior</em>), and it essentially means <span class="math notranslate nohighlight">\(P(ATE)\)</span> is a constant for all possible outcomes.</p> <p>If <span class="math notranslate nohighlight">\(P(ATE)\)</span> is a constant, then <span class="math notranslate nohighlight">\(P(ATE| X) \propto P(X| ATE) * c\)</span> for some constant <span class="math notranslate nohighlight">\(c\)</span>. This implies <span class="math notranslate nohighlight">\(P(ATE| X) \propto P(X| ATE)\)</span> (since constants drop out when doing proportionate comparisons).</p> <p>Now, I want to emphasize that assuming that all outcomes of <span class="math notranslate nohighlight">\(P(ATE)\)</span> are equally likely is a <em>very</em> weird thing to assume. After all, you’re doing an A/B test because you have some suspicion that your treatment will have an effect, right?</p> <p>But I think it’s helpful to consider this case as a way of understanding the relationship between Frequentist statistics like p-values and the quantities we often actually care about (like the probability that the Null hypothesis of no effect is true): namely, if we are willing to assume that the world started with the study we are conducting and ends with the study we are conducting, and that we know nothing except what is in our dataset, then these two <em>substantively and theoretically distinct</em> quantities will be the same.</p> </section> <section id="Reason-3:-The-Bernstein-von-Mises-Theorem"> <h3 id="Reason-3:-The-Bernstein-von-Mises-Theorem">Reason 3: The Bernstein-von Mises Theorem<a class=headerlink  href="#Reason-3:-The-Bernstein-von-Mises-Theorem" title="Link to this heading">¶</a></h3> <p>My reasoning 2 above is a <em>liiiiitle</em> hand-wavy. I think it’s entirely appropriate for applied use, and it’s a common perspective among many of my peers. However, it isn’t <em>quite</em> precise enough for a pure statistician. What <em>is</em> precise enough, however, is the <a class="reference external" href="https://en.wikipedia.org/wiki/Bernstein%E2%80%93von_Mises_theorem">Bernstein-von Mises Theory</a> which formalizes that logic for situations where your datasets get large (i.e. it’s an asymptotic proof). I’m not gonna try and go into, but I do want to link to it in case it interests you.</p> </section> </section> <section id=Doing-It-Like-A-Bayesian > <h2 id=Doing-It-Like-A-Bayesian >Doing It Like A Bayesian<a class=headerlink  href="#Doing-It-Like-A-Bayesian" title="Link to this heading">¶</a></h2> <p>And now, finally, I feel obligated to show you how you can do this using fully Bayesian machinery so we can compare results and you can see I’m not hiding anything under my hat.</p> <p>I’m doing this will a little hesitancy because, just as showing someone how to run a regression without explaining the assumptions that underlie it is a little like handing them a loaded gun, so too is showing you how to fit a Bayesian model without teaching you how Bayesian stats work in detail feels a little dangerous. But as the goal here is to give you a <em>sense</em> of how these things work, here we go.</p> <p>If you want to follow along, we’ll be using the <a class="reference external" href="https://bambinos.github.io/bambi/notebooks/getting_started.html">bambi package</a>, which provides an easy API for doing linear modelling using the library PyMC as a backend. To use it, install <code class="docutils literal notranslate"><span class=pre >bambi</span></code> and <code class="docutils literal notranslate"><span class=pre >arviz</span></code>. Note that Bayesian analysis comes with some pretty heavy duty computational libraries, so you may run into problems if you don’t install these packages in a new, clean environment.</p> <p>Also note that <code class="docutils literal notranslate"><span class=pre >bambi</span></code> <em>looks</em> like it uses formulas the way that <code class="docutils literal notranslate"><span class=pre >statsmodels</span></code> do, but the library doing it isn’t patsy, so you’ll find you have to make a few changes.</p> <div class="nbinput docutils container"> <div class="prompt highlight-none notranslate"><div class=highlight ><pre><span></span>[23]:
</pre></div> </div> <div class="input_area highlight-ipython3 notranslate"><div class=highlight ><pre><span></span><span class=kn >import</span> <span class=nn >bambi</span> <span class=k >as</span> <span class=nn >bmb</span>
<span class=kn >import</span> <span class=nn >arviz</span> <span class=k >as</span> <span class=nn >az</span>

<span class=c1 ># Prep vars since formula doesn't work the same way</span>
<span class=n >sales</span><span class=p >[</span><span class=s2 >"MarketID"</span><span class=p >]</span> <span class=o >=</span> <span class=n >pd</span><span class=o >.</span><span class=n >Categorical</span><span class=p >(</span><span class=n >sales</span><span class=p >[</span><span class=s2 >"MarketID"</span><span class=p >])</span>
<span class=n >sales</span><span class=p >[</span><span class=s2 >"log_AgeOfStore"</span><span class=p >]</span> <span class=o >=</span> <span class=n >np</span><span class=o >.</span><span class=n >log</span><span class=p >(</span><span class=n >sales</span><span class=p >[</span><span class=s2 >"AgeOfStore"</span><span class=p >])</span>
<span class=n >sales</span><span class=p >[</span><span class=s2 >"MarketID"</span><span class=p >]</span><span class=o >.</span><span class=n >dtype</span>

<span class=c1 ># Fit a linear model</span>
<span class=c1 ># with flat / uninformative priors.</span>
<span class=n >model</span> <span class=o >=</span> <span class=n >bmb</span><span class=o >.</span><span class=n >Model</span><span class=p >(</span>
    <span class=s2 >"total_sales ~ Promotion + log_AgeOfStore + MarketID"</span><span class=p >,</span>
    <span class=n >sales</span><span class=p >,</span>
    <span class=n >priors</span><span class=o >=</span><span class=p >{</span><span class=s2 >"common"</span><span class=p >:</span> <span class=n >bmb</span><span class=o >.</span><span class=n >Prior</span><span class=p >(</span><span class=s2 >"Flat"</span><span class=p >)},</span>
<span class=p >)</span>
<span class=n >results</span> <span class=o >=</span> <span class=n >model</span><span class=o >.</span><span class=n >fit</span><span class=p >(</span><span class=n >draws</span><span class=o >=</span><span class=mi >10_000</span><span class=p >,</span> <span class=n >chains</span><span class=o >=</span><span class=mi >4</span><span class=p >)</span>
<span class=n >az</span><span class=o >.</span><span class=n >summary</span><span class=p >(</span><span class=n >results</span><span class=p >)</span>
</pre></div> </div> </div> <div class="nboutput docutils container"> <div class="prompt empty docutils container"> </div> <div class="output_area stderr docutils container"> <div class=highlight ><pre>
Auto-assigning NUTS sampler...
Initializing NUTS using jitter+adapt_diag...
Multiprocess sampling (4 chains in 4 jobs)
NUTS: [total_sales_sigma, Intercept, Promotion, log_AgeOfStore, MarketID]
</pre></div></div> </div> <div class="nboutput docutils container"> <div class="prompt empty docutils container"> </div> <div class="output_area docutils container"> <script type="application/vnd.jupyter.widget-view+json">{"model_id": "96a47c02582c4a698e7e4b2eee2be29d", "version_major": 2, "version_minor": 0}</script></div> </div> <div class="nboutput docutils container"> <div class="prompt empty docutils container"> </div> <div class="output_area rendered_html docutils container"> <pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"></pre></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area rendered_html docutils container">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">
</pre></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class=highlight ><pre>
Sampling 4 chains for 1_000 tune and 10_000 draw iterations (4_000 + 40_000 draws total) took 8 seconds.
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class=highlight ><pre><span></span>[23]:
</pre></div>
</div>
<div class="output_area rendered_html docutils container">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border=1  class=dataframe >
<thead>
<tr style="text-align: right;">
<th>
<th>mean
<th>sd
<th>hdi_3%
<th>hdi_97%
<th>mcse_mean
<th>mcse_sd
<th>ess_bulk
<th>ess_tail
<th>r_hat



<tr>
<th>Intercept
<td>143.366
<td>5.028
<td>134.008
<td>152.991
<td>0.038
<td>0.027
<td>17146.0
<td>23705.0
<td>1.0

<tr>
<th>MarketID[2]
<td>103.607
<td>5.933
<td>92.444
<td>114.985
<td>0.040
<td>0.028
<td>22270.0
<td>27075.0
<td>1.0

<tr>
<th>MarketID[3]
<td>197.929
<td>4.781
<td>189.058
<td>207.141
<td>0.038
<td>0.027
<td>16222.0
<td>24485.0
<td>1.0

<tr>
<th>MarketID[4]
<td>73.436
<td>6.129
<td>61.705
<td>84.858
<td>0.040
<td>0.028
<td>23936.0
<td>29035.0
<td>1.0

<tr>
<th>MarketID[5]
<td>62.162
<td>5.627
<td>51.337
<td>72.605
<td>0.039
<td>0.028
<td>20781.0
<td>26505.0
<td>1.0

<tr>
<th>MarketID[6]
<td>3.976
<td>5.228
<td>-5.847
<td>13.878
<td>0.038
<td>0.027
<td>18780.0
<td>26073.0
<td>1.0

<tr>
<th>MarketID[7]
<td>37.410
<td>5.027
<td>27.855
<td>46.844
<td>0.038
<td>0.027
<td>17975.0
<td>26799.0
<td>1.0

<tr>
<th>MarketID[8]
<td>50.425
<td>5.065
<td>40.914
<td>60.084
<td>0.037
<td>0.026
<td>18380.0
<td>26934.0
<td>1.0

<tr>
<th>MarketID[9]
<td>66.364
<td>5.425
<td>56.299
<td>76.761
<td>0.039
<td>0.027
<td>19555.0
<td>26818.0
<td>1.0

<tr>
<th>MarketID[10]
<td>79.693
<td>5.256
<td>69.880
<td>89.649
<td>0.039
<td>0.027
<td>18565.0
<td>26965.0
<td>1.0

<tr>
<th>Promotion
<td>19.473
<td>2.392
<td>15.041
<td>24.037
<td>0.010
<td>0.007
<td>58978.0
<td>29622.0
<td>1.0

<tr>
<th>log_AgeOfStore
<td>-1.270
<td>1.329
<td>-3.768
<td>1.251
<td>0.006
<td>0.005
<td>56647.0
<td>28731.0
<td>1.0

<tr>
<th>total_sales_sigma
<td>10.698
<td>0.872
<td>9.110
<td>12.352
<td>0.004
<td>0.003
<td>43596.0
<td>30830.0
<td>1.0


</table>
</div></div>
</div>
<p>Now let’s compare the results from our bootstrap approach above to what we get from the Bayesian model.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class=highlight ><pre><span></span>[24]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class=highlight ><pre><span></span><span class=c1 ># Get sampled observations of the value of Promotion from Bambi.</span>
<span class=n >bambi_draws</span> <span class=o >=</span> <span class=n >results</span><span class=o >.</span><span class=n >posterior</span><span class=p >[</span><span class=s2 >"Promotion"</span><span class=p >]</span><span class=o >.</span><span class=n >loc</span><span class=p >[</span><span class=mi >0</span><span class=p >,</span> <span class=p >:]</span><span class=o >.</span><span class=n >values</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class=highlight ><pre><span></span>[25]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class=highlight ><pre><span></span><span class=c1 ># Now I'm going to sample from the posterior of the model</span>
<span class=n >pymc</span> <span class=o >=</span> <span class=n >pd</span><span class=o >.</span><span class=n >DataFrame</span><span class=p >(</span>
    <span class=p >{</span><span class=s2 >"estimated_ATE"</span><span class=p >:</span> <span class=n >bambi_draws</span><span class=p >,</span> <span class=s2 >"source"</span><span class=p >:</span> <span class=s2 >"posterior draws with flat priors"</span><span class=p >}</span>
<span class=p >)</span>
<span class=n >bootstrap</span> <span class=o >=</span> <span class=n >pd</span><span class=o >.</span><span class=n >DataFrame</span><span class=p >({</span><span class=s2 >"estimated_ATE"</span><span class=p >:</span> <span class=n >estimates</span><span class=p >,</span> <span class=s2 >"source"</span><span class=p >:</span> <span class=s2 >"bootstrap estimates"</span><span class=p >})</span>
<span class=n >all_draws</span> <span class=o >=</span> <span class=n >pd</span><span class=o >.</span><span class=n >concat</span><span class=p >([</span><span class=n >bootstrap</span><span class=p >,</span> <span class=n >pymc</span><span class=p >])</span>
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class=highlight ><pre><span></span>[26]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class=highlight ><pre><span></span><span class=kn >import</span> <span class=nn >seaborn.objects</span> <span class=k >as</span> <span class=nn >so</span>
<span class=kn >from</span> <span class=nn >matplotlib</span> <span class=kn >import</span> <span class=n >style</span>
<span class=kn >import</span> <span class=nn >matplotlib.pyplot</span> <span class=k >as</span> <span class=nn >plt</span>
<span class=kn >import</span> <span class=nn >warnings</span>

<span class=n >warnings</span><span class=o >.</span><span class=n >simplefilter</span><span class=p >(</span><span class=n >action</span><span class=o >=</span><span class=s2 >"ignore"</span><span class=p >,</span> <span class=n >category</span><span class=o >=</span><span class=ne >FutureWarning</span><span class=p >)</span>


<span class=p >(</span>
    <span class=n >so</span><span class=o >.</span><span class=n >Plot</span><span class=p >(</span><span class=n >all_draws</span><span class=p >,</span> <span class=n >x</span><span class=o >=</span><span class=s2 >"estimated_ATE"</span><span class=p >,</span> <span class=n >color</span><span class=o >=</span><span class=s2 >"source"</span><span class=p >)</span>
    <span class=o >.</span><span class=n >add</span><span class=p >(</span>
        <span class=n >so</span><span class=o >.</span><span class=n >Bars</span><span class=p >(</span><span class=n >alpha</span><span class=o >=</span><span class=mf >0.5</span><span class=p >),</span> <span class=n >so</span><span class=o >.</span><span class=n >Hist</span><span class=p >(</span><span class=n >stat</span><span class=o >=</span><span class=s2 >"density"</span><span class=p >,</span> <span class=n >common_bins</span><span class=o >=</span><span class=kc >True</span><span class=p >,</span> <span class=n >common_norm</span><span class=o >=</span><span class=kc >True</span><span class=p >)</span>
    <span class=p >)</span>
    <span class=o >.</span><span class=n >label</span><span class=p >(</span><span class=n >title</span><span class=o >=</span><span class=s2 >"Bootstrap Estimate Distribution v Posterior Samples (flat priors)"</span><span class=p >)</span>
    <span class=o >.</span><span class=n >theme</span><span class=p >({</span><span class=o >**</span><span class=n >style</span><span class=o >.</span><span class=n >library</span><span class=p >[</span><span class=s2 >"seaborn-v0_8-whitegrid"</span><span class=p >]})</span>
<span class=p >)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class=highlight ><pre><span></span>[26]:
</pre></div>
</div>
<div class="output_area docutils container">
<img alt="../_images/exercises_exercise_expected_value_29_0.png" class=no-scaled-link  src="../_images/exercises_exercise_expected_value_29_0.png" style="width: 730.15px; height: 378.25px;"/>
</div>
</div>
<p>See? Nearly the same!</p>
<p>Now again, a true Bayesian would never use Flat priors, and if I let Bambi pick what are called “weakly informative” priors (basically priors that way “I really don’t think there are really extreme values”), you’ll see we get something very similar, but with slightly more probability mass in the middle of the distribution:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class=highlight ><pre><span></span>[27]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class=highlight ><pre><span></span><span class=c1 ># If priors are not specified, weakly informative choices are made</span>
<span class=c1 ># https://mc-stan.org/rstanarm/articles/priors.html#default-weakly-informative-prior-distributions-1</span>

<span class=n >weakly_informative_prior_model</span> <span class=o >=</span> <span class=n >bmb</span><span class=o >.</span><span class=n >Model</span><span class=p >(</span>
    <span class=s2 >"total_sales ~ Promotion + log_AgeOfStore + MarketID"</span><span class=p >,</span>
    <span class=n >sales</span><span class=p >,</span>
<span class=p >)</span>
<span class=n >weakly_informative_prior_results</span> <span class=o >=</span> <span class=n >weakly_informative_prior_model</span><span class=o >.</span><span class=n >fit</span><span class=p >(</span>
    <span class=n >draws</span><span class=o >=</span><span class=mi >10_000</span><span class=p >,</span> <span class=n >chains</span><span class=o >=</span><span class=mi >4</span>
<span class=p >)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class=highlight ><pre>
Auto-assigning NUTS sampler...
Initializing NUTS using jitter+adapt_diag...
Multiprocess sampling (4 chains in 4 jobs)
NUTS: [total_sales_sigma, Intercept, Promotion, log_AgeOfStore, MarketID]
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "3f55dde3815f4cf3881930df67232125", "version_major": 2, "version_minor": 0}</script></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area rendered_html docutils container">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"></pre></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area rendered_html docutils container">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">
</pre></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class=highlight ><pre>
Sampling 4 chains for 1_000 tune and 10_000 draw iterations (4_000 + 40_000 draws total) took 8 seconds.
</pre></div></div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class=highlight ><pre><span></span>[28]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class=highlight ><pre><span></span><span class=n >weakly_informative_bambi_draws</span> <span class=o >=</span> <span class=p >(</span>
    <span class=n >weakly_informative_prior_results</span><span class=o >.</span><span class=n >posterior</span><span class=p >[</span><span class=s2 >"Promotion"</span><span class=p >]</span><span class=o >.</span><span class=n >loc</span><span class=p >[</span><span class=mi >0</span><span class=p >,</span> <span class=p >:]</span><span class=o >.</span><span class=n >values</span>
<span class=p >)</span>

<span class=n >weakly_informative_draws</span> <span class=o >=</span> <span class=n >pd</span><span class=o >.</span><span class=n >DataFrame</span><span class=p >(</span>
    <span class=p >{</span>
        <span class=s2 >"estimated_ATE"</span><span class=p >:</span> <span class=n >weakly_informative_bambi_draws</span><span class=p >,</span>
        <span class=s2 >"source"</span><span class=p >:</span> <span class=s2 >"Posterior Samples with Weakly Informative Estimates"</span><span class=p >,</span>
    <span class=p >}</span>
<span class=p >)</span>
<span class=n >all_draws</span> <span class=o >=</span> <span class=n >pd</span><span class=o >.</span><span class=n >concat</span><span class=p >([</span><span class=n >all_draws</span><span class=p >,</span> <span class=n >weakly_informative_draws</span><span class=p >])</span>
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class=highlight ><pre><span></span>[29]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class=highlight ><pre><span></span><span class=p >(</span>
    <span class=n >so</span><span class=o >.</span><span class=n >Plot</span><span class=p >(</span>
        <span class=n >all_draws</span><span class=p >[</span><span class=n >all_draws</span><span class=o >.</span><span class=n >source</span> <span class=o >!=</span> <span class=s2 >"posterior draws with flat priors"</span><span class=p >],</span>
        <span class=n >x</span><span class=o >=</span><span class=s2 >"estimated_ATE"</span><span class=p >,</span>
        <span class=n >color</span><span class=o >=</span><span class=s2 >"source"</span><span class=p >,</span>
    <span class=p >)</span>
    <span class=o >.</span><span class=n >add</span><span class=p >(</span>
        <span class=n >so</span><span class=o >.</span><span class=n >Bars</span><span class=p >(</span><span class=n >alpha</span><span class=o >=</span><span class=mf >0.5</span><span class=p >),</span> <span class=n >so</span><span class=o >.</span><span class=n >Hist</span><span class=p >(</span><span class=n >stat</span><span class=o >=</span><span class=s2 >"density"</span><span class=p >,</span> <span class=n >common_bins</span><span class=o >=</span><span class=kc >True</span><span class=p >,</span> <span class=n >common_norm</span><span class=o >=</span><span class=kc >True</span><span class=p >)</span>
    <span class=p >)</span>
    <span class=o >.</span><span class=n >label</span><span class=p >(</span>
        <span class=n >title</span><span class=o >=</span><span class=s2 >"Bootstrap Estimate Distribution v Posterior Samples with Weakly Informative Priors"</span>
    <span class=p >)</span>
    <span class=o >.</span><span class=n >theme</span><span class=p >({</span><span class=o >**</span><span class=n >style</span><span class=o >.</span><span class=n >library</span><span class=p >[</span><span class=s2 >"seaborn-v0_8-whitegrid"</span><span class=p >]})</span>
<span class=p >)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class=highlight ><pre><span></span>[29]:
</pre></div>
</div>
<div class="output_area docutils container">
<img alt="../_images/exercises_exercise_expected_value_33_0.png" class=no-scaled-link  src="../_images/exercises_exercise_expected_value_33_0.png" style="width: 859.775px; height: 378.25px;"/>
</div>
</div>
</section>
<section id="Still-Reading?">
<h2 id="Still-Reading?">Still Reading?<a class=headerlink  href="#Still-Reading?" title="Link to this heading">¶</a></h2>
<p>Then you should take a class on applied Bayesian statistics. It’s very cool, and honestly just makes way, <em>way</em> more sense than Frequentist stats. And now that computers are superfast, we can do Bayesian statistics <em>far</em> more easily than we could just a decade ago.</p>
</section>
</section>


          </article>
        </div>
      </div>
    </main>
  </div>
  <footer class=md-footer >
    <div class=md-footer-nav >
      <nav class="md-footer-nav__inner md-grid">
          
          
        </a>
        
      </nav>
    </div>
    <div class="md-footer-meta md-typeset">
      <div class="md-footer-meta__inner md-grid">
        <div class=md-footer-copyright >
          <div class=md-footer-copyright__highlight >
              &#169; Copyright 2022, Nick Eubank.
              
          </div>
            Created using
            <a href="http://www.sphinx-doc.org/">Sphinx</a> 7.2.6.
             and
            <a href="https://github.com/bashtage/sphinx-material/">Material for
              Sphinx</a>
        </div>
      </div>
    </div>
  </footer>
  <script src="../_static/javascripts/application.js"></script>
  <script>app.initialize({version: "1.0.4", url: {base: ".."}})</script>