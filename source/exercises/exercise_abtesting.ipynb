{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A/B Testing the Udacity Website\n",
    "\n",
    "In these exercises, we'll be analyzing data on user behavior during an A/B test of the onboarding process for Udacity (Udacity generously provides this data under an Apache open-source license [here](https://www.kaggle.com/tammyrotem/ab-tests-with-python/notebook). Also, if you're interested in learning more on A/B testing, it seems only fair while we use their data to flag they have a full course on the subject [here](https://www.udacity.com/course/ab-testing--ud257). \n",
    "\n",
    "The test [is described as follows](https://www.kaggle.com/tammyrotem/ab-tests-with-python/notebook): \n",
    "\n",
    "At the time of this experiment, Udacity courses currently have two options on the course overview page: \"start free trial\", and \"access course materials\".\n",
    "\n",
    "**Current Conditions Before Change**\n",
    "\n",
    "- If the student clicks \"start free trial\", they will be asked to enter their credit card information, and then they will be enrolled in a free trial for the paid version of the course. After 14 days, they will automatically be charged unless they cancel first.\n",
    "- If the student clicks \"access course materials\", they will be able to view the videos and take the quizzes for free, but they will not receive coaching support or a verified certificate, and they will not submit their final project for feedback.\n",
    "\n",
    "**Description of Experimented Change**\n",
    "\n",
    "- In the experiment, Udacity tested a change where if the student clicked \"start free trial\", they were asked how much time they had available to devote to the course.\n",
    "- If the student indicated 5 or more hours per week, they would be taken through the checkout process as usual. If they indicated fewer than 5 hours per week, a message would appear indicating that Udacity courses usually require a greater time commitment for successful completion, and suggesting that the student might like to access the course materials for free.\n",
    "- At this point, the student would have the option to continue enrolling in the free trial, or access the course materials for free instead. This [screenshot](images/udacity_checkyoureready.png) shows what the experiment looks like.\n",
    "\n",
    "**Udacity's Hope is that...**:\n",
    "\n",
    "> this might set clearer expectations for students upfront, thus reducing the number of frustrated students who left the free trial because they didn't have enough time -- without significantly reducing the number of students to continue past the free trial and eventually complete the course. If this hypothesis held true, Udacity could improve the overall student experience and improve coaches' capacity to support students who are likely to complete the course.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "Begin by importing Udacity's data on user behavior by going to http://www.github/nickeubank/MIDS_Data/ and using the `udacity_AB_testing`folder, or by clicking [here.](https://github.com/nickeubank/MIDS_Data/tree/master/udacity_AB_testing) Note that there are TWO datasets for this test -- one for the control data (users who saw the original design), and one for treatment data (users who saw the experimental design). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "\n",
    "Explore the data. Can you identifying the unit of observation of the data (e.g. what is represented by each row)? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "\n",
    "The easiest way to analyze this data is to stack it into a single dataset where each observation is a day-treatment-arm (so you should end up with two rows per day, one for those who are in the treated groups, and one for those who were in the control group). Note that currently nothing in the data identifies whether a given observation is a treatment group observation or a control group observation, so you'll want to make sure to add a \"treatment\" indicator variable.\n",
    "\n",
    "The variables in the data are:\n",
    "\n",
    "- Pageviews: number of unique users visiting homepage\n",
    "- Clicks: number of those users clicking \"Start Free Trial\"\n",
    "- Enrollments: Number of people enrolling in trial\n",
    "- Payments: Number of people who eventually pay for the service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4\n",
    "\n",
    "Whenever you are working with experimental data, the first thing you want to do is verify that users actually were randomly sorted into the two arms of the experiment. In this data, half of users were supposed to be shown the old version of the site and half were supposed to see the new version. \n",
    "\n",
    "`Pageviews` tells you how many unique users visited the welcome site we are experimenting on. `Pageviews` is what is sometimes called an \"invariant\" variable, meaning that it shouldn't vary across treatment arms -- after all, people have to visit the site before they get a chance to see the treatment, so there's no way that being assigned to treatment or control should affect the number of pageviews assigned to each group. \n",
    "\n",
    "This is also what's known as a \"pre-treatment\" variable, because it was determined before users were manipulated in any way. That makes it analogous to gender or age in experiments where you have demographic data -- a person's age and gender are determined before they experience any manipulations, so the value of any invariant / pre-treatment attributes should be the same across the two arms of our experiment. This is what is called \"checking for balance.\" If pre-treatment attributes aren't balanced, then we know our attempt to randomly assign people to different groups failed.\n",
    "\n",
    "To test the quality of the randomization, calculate the average number of pageviews for the treated group and for the control group. Do they look similar? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5\n",
    "\n",
    "\"Similar\" is a tricky concept -- obviously, we expect *some* differences across groups. The question is whether the differences between groups are larger than we'd expect to emerge randomly. To evaluate this, let's use a `ttest`. \n",
    "\n",
    "If you're using R, just use `t.test`. \n",
    "\n",
    "If you're using Python, you can use the `ttest` function from scipy, which you can import as `from scipy.stats import ttest_ind`. \n",
    "\n",
    "**Note**: Remember that scipy functions don't accept `pandas` objects, so you have to pass the numpy vectors underlying your data with the `.values` operator (e.g. `df.my_column.values`). \n",
    "\n",
    "Does the difference in `pageviews` look statistically significant?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6\n",
    "\n",
    "`Pageviews` is not the only pre-treatment variable in this data. What other measure is pre-treatment? Review the description of the experiment if you're not sure. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 7\n",
    "\n",
    "Check if the other pre-treatment variable is also balanced.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 8\n",
    "\n",
    "Now that we've established we have good balance (meaning we think randomization was likely successful), we can evaluate the effects of the experiment. Decide on two metrics you want to use to evaluate the success of the trial, and test whether those values are different in the control group and treatment group. Because we've randomized, this is a consistent estimate of the Average Treatment Effect of Udacity's website change. \n",
    "\n",
    "What is the ATE on Enrollments, and what is the ATE on Payments? Did Udacity achieve their goal?\n",
    "\n",
    "**Note:** You may discover some issues with your data. Can you figure out what's going on, and adjust?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 9\n",
    "\n",
    "One of the magic things about experiments is that all you have to do is compare averages to get an average treatment effect. However, you *can* do other things to try and increase the statistical power of your experiments, like add controls in a linear regression model. \n",
    "\n",
    "As you likely know, a bivariate regression is exactly equivalent to a t-test, so let's start by re-estimating the effect of treatment on `Payments` using a linear regression. Can you replicate the results from your t-test?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 10\n",
    "\n",
    "Now add indicator variables for the day of each observation. Do the standard errors on your `treatment` variable change? If so, in what direction?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 11\n",
    "\n",
    "You should have found that your standard errors decreased by about a third -- this is why, although just comparing means *works*, if you have additional variables you should add them as covariates in your analysis. Moreover, in other settings you may find this effect is even larger -- the date indicators we added to our data are perfectly balanced between treatment and control, so we aren't adding a lot of data to the model by adding them as variables. As we'll see in later exercises, adding variables like \"gender\" or \"age\" (which will never be perfectly balanced across treatment and control) will help even more. \n",
    "\n",
    "As a last exercise, instead of adding indicators for each date, add indicators for *day of the week* (e.g. Monday, Tuesday, etc.). This is just for data manipulation practice!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
