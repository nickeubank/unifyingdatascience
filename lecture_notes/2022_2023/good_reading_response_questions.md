Good Questions:


Paul:
I have a couple related questions about the relationship between matching (via propensity score or whatever method) and controlling for variables. If you have a perfectly matched 1:1 dataset for all treatment arms with all the variables we collected/have reason to believe influence the outcome of interest, is it necessary to control for those variables that have already been matched? Additionally, is it ever bad (or possible) to over-match or over-control? I feel like the answer for matching is no, matching on as many possible things is ideal because then we can rule out as many other possible explanations, but I've heard/read somewhere that over-controlling is actually a statistical mistake? Am I misremembering or can you go into this a bit?


Athena:
While indicator variables are convenient in representing categorical variables, I'm concerned about dimensionality. When working with a dataset that contains predominantly categorical variables, each category contains more than 10 types of values, the dimension of the One-Hot-Encoded dataset could increase drastically. Not to mention that the interpretability of the model decreases, the runtime of the model could increase as well. Or, maybe the runtime wouldn't be affected as much because most fields are binary? I want to know what should we do to decrease runtime and increase interpretability when we are working on high dimensional One-Hot-Encoded dataset. And, are there other encoding methods that we could use to avoid increasing the dimensionality of the dataset.

Marlyne: When talking about fixed effects, there is a group level that's controlled (for example, controlling for schools). I seem to recall that hierarchical models have multiple levels such as Country level, followed by State level, followed by county. Could we do a similar breakdown on fixed effects (more than one level), if so, how would that translate into a regression equation/results? Would we do that by just adding another indication variable?

Erika: I can't help but comment that the fact that the students chose University of Texas/University of Virginia over private schools means absolutely nothing because those schools are also about as rigorous and competitive as it gets... coming from someone who is very familiar with UT. It would be way more interesting to see someone with high test scores (could maybe get into Harvard, but I don't actually care if they get into Harvard because unlike how this reading implies admission is not the end-all be-all reading of someone being able to succeed there) that chose to attend a community college instead. I can see that the cost is lower for a place like UT, and they are looking to see if that impact success, but I look at that and think what does that matter if the experience is mostly the same (the only argument I could see that maybe the UT student has less financial stress, therefore is MORE successful than the Harvard student). This is all to say that this reading made me realize how difficult pinpointing causal inference can be in some cases, and that there may be cases where it literally just can't be done. What we did in the opioid project last semester seems like a sound way to judge causal inference by looking at a different state, but looking at individuals when it comes to college success seems so much more finicky and impossible to grasp. They mentioned that many college students meet spouses- I have read that being married also helps success so that could be the cause instead of Harvard. I guess the question I have is, there seem to be so many cases where finding cause is infeasible, but without a doubt people still try- are they in the wrong here? Did they skip the "is this feasible?" step, or are they times where can study the changes, and just accept that we can't attribute cause?