Discrimination

-----------
- Hausman test
  - https://tedboy.github.io/statsmodels_doc/generated/statsmodels.sandbox.regression.gmm.spec_hausman.html
- Dani / Naman: What are hierarchical models?
- Dauren: Why cluster?
- Naman: how many interactions
- Surabhi: What exactly is panel data?
- Anna: On an unrelated note, I am also curious about the `.get_robustcov_results('HC3’)` command. I had tried to look up the meanings but I’ve only found that it returns a more robust covariance and I’m not sure I understand what that means. Should we use this command as default? If not, how do we know when it’s appropriate and which HC number we should use?

- Michelle: One thing I found confusing was the dummy variables and their implementation . In the first reading, you use the C(variable) in the smf linear regression to convert it to a categorical variable which can be used in the linear regression. The C{} is analogous to to astype('category') in python. However, last weekend I participated in a datathon where I had to model a logistic regression with multiple categorical variables. Using astype('category') did not work and actually caused an error - ValueError: could not convert string to float. It seemed that changing it to categorical variables, even though it should be automatic, was unable to change the variable to the specific type needed for logistic regression. As a solution, I used two methods, one is using LabelEncoding and using pd.get_dummies on the dataframe to create dummy variables and that worked. So what I am confused about is if using C() is one approach in creating dummy variables for our model, then why did it not work when using logistic regression but using pd.get_dummies did? DoeC() and as type('category') only change the type of the data type while pd.get_dummies create new columns in the dataframe for each level of the predictors - so does that mean using C() doesnt really actually create dummy variables?



