- chatGPT/copilot
- OHs
- Reading Reflections

- Passive-predictive v. causal (Lorna):

I am confused about how predictive questions are not considered a subset of casual questions. This is primarily due to the framing of casual questions such as, "How does X affect Y?”  However, imagining that answer requires us to consider a predictive question of whether Y will be the same or different with X present. The distinction between the two types of questions does not stand out for me, which leads me to the question of whether or not there is an interplay between them.

- f v. Exploratory:

The one thing I found confusing about the reading was the differences between exploratory data analysis and exploratory questions data scientists should ask about the data. The reading did a good job emphasizing just how necessary it is for data scientists to ask questions about the bigger picture and diversify approaches to handling data science questions. But I found it hard to distinguish between EDA and answering exploratory questions. It seems to me that one is a part of another, but I'd love to hear more about the nuisances of each approach.

- Bias (Xiaoquan):

Since supervised machine learning algorithms inherit the bias of whoever labeled the training data, how do we use them in human-bias fields like hiring? Is that to say that we cannot trust ML models in those fields? Is it possible to have a certain algorithm that can perfectly answer our questions and ignore those signals that may cause bias, or are there other ways to mitigate inherited human bias in models?

- Song:

What’s a little confusing in the reading is that a supervised machine learning model only learns how to emulate the behavior of the humans who label the training dataset. Then, I would like to know if there is no way that the humans’ mistakes can be corrected during the model’s learning process. In my opinion, the main reason why we invest a lot of things in developing ML models would be that we expect them to be better than humans. If they just end up copying we humans’ behavior and inheriting our biases, it’s quite disappointing.

- Ahmed:

While the reading was definitely intriguing and eye opening as to understanding how to frame a data science problem, I wanted to understand whether this is the approach that all data scientists take when they try to solve any data science problems, i.e. designing exploratory questions, passive prediction questions, and causal questions. In particular, I’m interested to know whether this is the approach that all data scientists take (including ML engineers/data analysts/statisticians) take or is it an approach applicable only in the causal inference domain?

## Killer Example

- Yuanjing:

The reading groups data science questions into 3 groups: descriptive/exploratory questions, passive predictive questions, and causal questions. What I found most confusing is how to determine which class of question to ask and how many questions need to be answered would be sufficient to solve our problem. For example, in the case of an advertising dataset from the book An Introduction to Statistical Learning, the company wants to understand the relationship between sales and media categories. It asks three questions: "Which media are associated with sales?", "Which media generate the biggest boost in sales?", "How large of an increase in sales is associated with a given increase in TV advertising?". These are exploratory questions. I am unsure why only exploratory questions were asked and whether there are other questions that should be asked to fully understand the problem.  Additionally, I am curious if it's possible to use multiple types of questions together in a data science project and if so, is there a specific order in which they should be answered. My intuition is that a data science project answers multiple types of questions. For instance, if a data analysis aims to answer an passive prediction question, exploratory questions must also be answered during the process of answering the passive prediction question. I would not jump straight into passive prediction / causal inference without having determined the overall distributions of the data.

- Ya-Yun

What makes me confused is that the news mentioned in previous question seems to make general public feel that the distribution of kidney transplants is biased, problematic or even racist. However as a doctor, I know this is exactly how doctors were taught to evaluate these transplant candidates. Race is indeed an important factor to consider while making lots of medical decisions. Thus I couldn’t be sure whether this kidney-scoring-formula or those decisions that the algorithm made were problematic. It’s critical to first clarified “how” this scoring formula incorporating race was initially built? Does it truly indicate that Black patients would have healthier kidneys so that they do not need  kidney transplant as urgent as white patients, and why ? I believe this is when “casual inference” would come in handy

- Andrew:

I would like to see a deeper discussion into the concept of "AI bias" at some point this semester, in this course or 705. I realize I am lumping a lot of concepts into "AI bias," but here's a synopsis:

-"Social media recommendation engines are likely to reinforce behavior that leads to extremism." This makes sense, and should not be controversial. There's also some irony captured in here, but we'll let it slide for now.
-"Supervised Machine Learning models capture the biases of the supervisors." Also not surprising, and we need to get better as a profession at steering the way we define this issue.

It's the "everything else" that I would like to see discussed. There are plenty of cases where we hear of things like a chatbot that has gone sideways and the default is to blame AI, the people who built it, or a computer like it has the ability to understand what is actually happening. This doesn't seem right, and I want to have the tools to have this discussion with people so that we can educate others.

- Kashaf

While it made sense how the reading divided data science questions into three groups; descriptive/exploratory, passive predictive, and causal questions, I was confused about whether there is a significant overlap between passive predictive and causal questions, and the tools or models used to answer them. Moreover, I also wanted to understand if these two kinds of questions are complementary, i.e. you need to answer one to help gain a better understanding of the other one. Passive prediction questions allow us to predict the effect of a change we deliberately introduce in the world. For example, given this new customer behavior on the website, how likely are customers to spend a lot over the next year? On the other hand, causal questions analyze the effect of an action on its outcome, for instance, if we change this feature of the website, how does it change consumer spending? While there are slight differences between the two types of questions, I was curious whether it is possible to use the same model that allows us to predict how consumer spending will change to also identify which factor caused that change. Based on my previous professional experiences, I believe that sometimes in real-life business scenarios, prediction alone might not be sufficient to make effective decisions (passive prediction), unless it is possible to understand the reason behind why a specific outcome was observed (causal inference).
