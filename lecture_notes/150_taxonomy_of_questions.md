
Dani: 
    - I am a bit confused about how the distinction between how the world is versus how the world should be is blurred in industry. I feel like in my past statistics courses, building models is deemed as an amazing tool that will tell us what we should do, what we should prioritize, etc... but when has a business ever listened to one guy who has a different idea? That just seems like a bad business practice (listening to just any joe shmoe who threw some data in ML model and came up with an idea of what should be done). 
    - I guess my confusion comes from how this distinction is blurred in the real world and how we as data scientists are supposed to fix it. Part of me thinks that the feasibility step at the start of the project would be best at managing expectations and defining success (as in, "I can't precisely tell you what you should do"), but if the business wants someone to tell them what they should do, aren't they just gonna keep looking to find a joe shmoe?




Sarwari: 
    -In thinking about causality and prediction without manipulation, given that a lot of ML rests on the i.i.d assumption it feels fair to say that we shouldn't try to generalize machine learning models to problems beyond their domain. For example, the last 2 years saw many financial predictive models fail because they had never accounted for something like a pandemic, and hence could no longer make reliable predictions about the same. So I have two questions about such scenarios: a) Is there anything like a 'causal model' of prediction? It feels like a lot of causal inference happens in hindsight b) in Kyle's class, we've been introduced to (but haven't studied) 'reinforcement learning' as a type of ML algorithm. Would this be a reasonable solution to generalizing problems out of sample (at the risk of making wrong predictions in the starting period)?

Erika:
    - I'm confused on the difference between how we are using the term "descriptive questions" for this class and how it is sometimes used to refer to what we call "positive questions". You say that positive questions refer to how the world is, and that these are sometimes called descriptive questions, but we won't because this is different that our definition, which is questions about current or past state of the world. But questions referring to how the world is and questions about the current state of the world sound like the same thing?

Sydney: 
    - In the section on positive and normative questions when it says assessing the desirability of different outcomes “can only be done… on the basis of a system of values”, who determines those values and what do you do when there are conflicting values?  With the example given, it sounds like the process of enacting the policy or not will depend on what a policymaker decides to do with the information given.  When I was working as a data analyst on a team of policymakers, I would work really hard to provide the team the data to help guide their decisions, and then give my recommendation on a path forward. I could see that providers were not complying with the new regulation if we were still paying them, but the policy team feared that if we stopped paying the providers, the Medicaid patients would stop receiving medical services.  There was a conflict of values. I valued our compliance numbers because it meant the Department would still receive federal funding, but their value laid heavily on the Medicaid members.  Either decision came at a cost, literally and figuratively. I’m sure this happens all the time with any kind of decision a company or organization needs to make, so what is our role as data scientists? Should we insert our opinions after we’ve presented the facts, even though it may not make a difference to the people in charge?